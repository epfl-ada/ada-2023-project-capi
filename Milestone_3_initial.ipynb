{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA CAPI Notebook for Project Milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "The structure of this notebook is modeled on the outline of the one specified in the README.\n",
    "1. Data Processing and Exploration\n",
    "    1. Load Tabular Data\n",
    "    2. Extracting metrics from textual articles\n",
    "    3. Data Exploration\n",
    "2. Data Analysis: Addressing the research subquestions\n",
    "    1. Categories\n",
    "    2. Analysing Article Metrics\n",
    "    3. Path Difficulty\n",
    "    4. Individual Player Behaviour\n",
    "3. Data Analysis: Putting Everything Together\n",
    "4. Machine Learning\n",
    "5. Interactive Plots for the Data Story\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Helper functions from utils folder\n",
    "from utils.analysis import (\n",
    "    t_test_article_metrics,\n",
    "    visualize_article_connections_per_category,\n",
    "    sorted_category_counts,\n",
    "    shortest_path_find,\n",
    "    simple_t_test,\n",
    "    bootstrap_CI_prob_cat,\n",
    "    create_coefplot,\n",
    "    evaluate_predictions,\n",
    ")\n",
    "from utils.preprocessing import (\n",
    "    merge_articles_categories,\n",
    "    create_category_dictionaries,\n",
    "    filter_games,\n",
    "    get_backclicked_pages,\n",
    "    country_codes_dict,\n",
    ")\n",
    "\n",
    "# Formatting libraries\n",
    "import urllib\n",
    "import datetime as datetime\n",
    "\n",
    "# Plotting libraries\n",
    "import gravis as gv\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "default_colors = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "# Imports to perform article analysis\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"punkt\")  # Punkt tokenizer\n",
    "nltk.download(\"stopwords\")  # Common stopwords\n",
    "\n",
    "# Regression libraries\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Stats libraries\n",
    "from statistics import mean, median\n",
    "\n",
    "# Load config and extract variables\n",
    "import config\n",
    "\n",
    "DATA_PATH = config.PATH_TO_DATA\n",
    "PATH_GRAPH_FOLDER = config.PATH_GRAPH_FOLDER\n",
    "ARTICLE_FOLDER = config.ARTICLE_FOLDER\n",
    "GENERATED_METRICS = config.GENERATED_METRICS\n",
    "DATASTORY_PLOTS = config.DATASTORY_PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 1 - Data Processing and Exploration\n",
    "\n",
    "Note that you can load the data from [here](#checkpoint1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabu\"></a>\n",
    "### 1.1 - Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all data (except wikipedia articles)\n",
    "finished_paths = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"paths_finished.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    skiprows=15,\n",
    "    names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"rating\"],\n",
    ")\n",
    "unfinished_paths = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"paths_unfinished.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    skiprows=16,\n",
    "    names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"target\", \"type\"],\n",
    ")\n",
    "edges = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"links.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    skiprows=15,\n",
    "    names=[\"start\", \"end\"],\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "articles = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"articles.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    skiprows=12,\n",
    "    names=[\"article\"],\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "categories = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"categories.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    skiprows=13,\n",
    "    names=[\"article\", \"category\"],\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "shortest_paths = np.genfromtxt(\n",
    "    os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"shortest-path-distance-matrix.txt\"),\n",
    "    delimiter=1,\n",
    "    dtype=np.uint8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "#### Clean Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding in edge list\n",
    "display(edges.head())\n",
    "edges[\"start\"] = edges.start.apply(urllib.parse.unquote)\n",
    "edges[\"end\"] = edges.end.apply(urllib.parse.unquote)\n",
    "display(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datetime as datetime object\n",
    "finished_paths[\"datetime\"] = finished_paths.timestamp.apply(\n",
    "    datetime.datetime.fromtimestamp\n",
    ")\n",
    "unfinished_paths[\"datetime\"] = unfinished_paths.timestamp.apply(\n",
    "    datetime.datetime.fromtimestamp\n",
    ")\n",
    "display(unfinished_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for articles\n",
    "display(articles.head())\n",
    "articles[\"article\"] = articles.article.apply(urllib.parse.unquote)\n",
    "display(articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for categories\n",
    "display(categories.head())\n",
    "categories[\"article\"] = categories.article.apply(urllib.parse.unquote)\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify broad categories of articles\n",
    "display(categories.head())\n",
    "categories[\"broad_category\"] = categories[\"category\"].apply(\n",
    "    lambda x: x.split(\".\")[1]\n",
    ")  # first entry after subject.\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge articles and categories\n",
    "articles_categories = pd.merge(articles, categories, how=\"left\", on=\"article\")\n",
    "display(articles_categories.head())\n",
    "\n",
    "# 6 articles without category!\n",
    "print(\n",
    "    \"Merge introduced {} NAs in category columns:\".format(\n",
    "        articles_categories.category.isna().sum()\n",
    "    )\n",
    ")\n",
    "articles_categories[articles_categories.category.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paths to a readable format (lists) and remove url encoding\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(\n",
    "    lambda x: [urllib.parse.unquote(y) for y in x]\n",
    ")\n",
    "\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(\n",
    "    lambda x: [urllib.parse.unquote(y) for y in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and target articles of path\n",
    "finished_paths[\"start\"] = [path[0] for path in finished_paths[\"path\"]]\n",
    "finished_paths[\"target\"] = [path[-1] for path in finished_paths[\"path\"]]\n",
    "\n",
    "unfinished_paths[\"start\"] = [path[0] for path in unfinished_paths[\"path\"]]\n",
    "unfinished_paths[\"target\"] = unfinished_paths[\"target\"].apply(urllib.parse.unquote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gen\"></a>\n",
    "\n",
    "### 1.2 - Extracting metrics from textual articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was used to access the `plaintext_articles` folder and read all articles inside, creating a dataframe with all the metric information (see table below). To reduce runtime, we compute the article metrics once and then read the generated CSV file.\n",
    "\n",
    "\n",
    "Article Metrics DataFrame Description:\n",
    "| Metric             | Metric Name                  | Description                                                                                                   |\n",
    "|-------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| `word_count`            | Total Word Count        | Represents the total word count in the article.                                                                |\n",
    "| `stopword_count`        | Stopword Count          | Measures the number of words that do not contribute significantly to the content's meaning.                 |\n",
    "| `stopword_percentage`   | Stopword Frequency      | Measures the percentage of words that do not contribute significantly to the content's meaning.                 |\n",
    "| `non_stopword_count`    | Non-Stopword Count      | Measures the number of words that contribute to the content's meaning.                              |\n",
    "| `non_stopword_percentage`| Non-Stopword Frequency  | Measures the percentage of words that contribute to the content's meaning.                              |\n",
    "| `avg_word_length`       | Average Word Length     | Calculates the average word length (in characters) in the article.                                               |\n",
    "| `avg_sent_length`       | Average Sentence Length | Calculates the average sentence length (in characters) in the article.                                               |\n",
    "| `paragraph_count`       | Number of Paragraphs    | Calculates the total number of paragraphs in the article.                                                        |\n",
    "| `common_words`          | Keyword Frequency       | Extracts the 10 most frequent of keywords.                  |\n",
    "| `readability_score`     | Readability (Flesch Score)| Utilizes the Flesch Reading Ease Score for assessing readability. [Learn more](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_article(article_text):\n",
    "    \"\"\"\n",
    "    Lower casing and formatting line splits.\n",
    "    :param article_text: Text of the article.\n",
    "    :return: Preprocessed text of the article.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed_text = article_text\n",
    "    preprocessed_text = preprocessed_text.lower()\n",
    "    preprocessed_text = preprocessed_text.replace(\n",
    "        \"\\n   \", \" \"\n",
    "    )  # As the articles are not continuous sentences\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def calculate_article_metrics(article_text):\n",
    "    \"\"\"\n",
    "    Calculate article metrics for a given article.\n",
    "    :param article_text: Text of the article.\n",
    "    :return: Dictionary of article metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    preprocessed_text = proprocess_article(article_text)\n",
    "\n",
    "    words = word_tokenize(preprocessed_text)\n",
    "    sentences = sent_tokenize(preprocessed_text)\n",
    "\n",
    "    # Calculate total word count\n",
    "    total_word_count = len(words)\n",
    "\n",
    "    # Calculate stopword frequency\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stopwords_count = 0\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word.isalpha() and word.lower() in stop_words:\n",
    "            stopwords_count += 1\n",
    "        if word.isalpha() and word.lower() not in stop_words:\n",
    "            unique_words.append(word.lower())\n",
    "\n",
    "    # Calculate average word length\n",
    "    average_word_length = sum(len(word) for word in words) / total_word_count\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    average_sentence_length = sum(len(sentence) for sentence in sentences) / len(\n",
    "        sentences\n",
    "    )\n",
    "\n",
    "    # Calculate number of paragraphs (assume every new line \\n is paragraph)\n",
    "    paragraphs_count = preprocessed_text.count(\"\\n\") + 1  # Count last paragraph\n",
    "\n",
    "    # Calculate keyword frequency\n",
    "    word_freq = nltk.FreqDist(unique_words)\n",
    "    most_common_words = word_freq.most_common(10)  # Parameter to adjust\n",
    "\n",
    "    # Calculate readability (Flesch Reading Ease Score) - 100: Easy to read, 0: Very confusing\n",
    "    readability = textstat.flesch_reading_ease(preprocessed_text)\n",
    "\n",
    "    return {\n",
    "        \"word_count\": total_word_count,\n",
    "        \"non_stopword_count\": total_word_count - stopwords_count,\n",
    "        \"non_stopword_percentage\": (total_word_count - stopwords_count)\n",
    "        / total_word_count,\n",
    "        \"stopword_count\": stopwords_count,\n",
    "        \"stopword_percentage\": stopwords_count / total_word_count,\n",
    "        \"avg_word_length\": average_word_length,\n",
    "        \"avg_sent_length\": average_sentence_length,\n",
    "        \"paragraph_count\": paragraphs_count,\n",
    "        \"common_words\": most_common_words,\n",
    "        \"readability_score\": readability,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not run this code since it takes while, the output is saved in \"/generated_data\"\n",
    "\n",
    "# folder_path = os.path.join(DATA_PATH, ARTICLE_FOLDER)\n",
    "# if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "\n",
    "#   article_metrics = pd.DataFrame(columns=[\"article\", \"word_count\", \"non_stopword_count\", \"non_stopword_percentage\", \"stopword_count\", \"stopword_percentage\", \"avg_word_length\", \"avg_sent_length\", \"paragraph_count\", \"common_words\", \"readability_score\"])\n",
    "\n",
    "#   for file_name in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "#     if os.path.isfile(file_path):\n",
    "#       root, extension = os.path.splitext(file_name)\n",
    "#       readable_file_name = urllib.parse.unquote(root)\n",
    "      \n",
    "#       with open(file_path, \"r\", encoding=\"utf-8\") as article:\n",
    "#         metrics = calculate_article_metrics(article.read())\n",
    "\n",
    "#         metrics[\"article\"] = readable_file_name\n",
    "#         article_metrics.loc[len(article_metrics)] = metrics\n",
    "# else:\n",
    "#   raise FileNotFoundError(\"The specified folder path does not exist or is not a directory.\")\n",
    "\n",
    "# article_metrics.to_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the article data (as mentioned before, this is done to reduce runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_metrics = pd.read_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_metrics.info())\n",
    "display(article_metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initial_exploration\"></a>\n",
    "\n",
    "### 1.3 - Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path Lengths\n",
    "Compare the path lengths between the finished and unfinished paths to detect potential outliers or trends that might influence the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate path lengths for finished paths and show summary statistics\n",
    "finished_paths[\"path_length\"] = finished_paths.path.apply(lambda el: len(el))\n",
    "finished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate path lengths for unfinished paths and show summary statistics\n",
    "unfinished_paths[\"path_length\"] = unfinished_paths.path.apply(lambda el: len(el))\n",
    "unfinished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of finished and unfinished paths\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "threshold = 40  # for now we remove some outliers to make the plots meaningful\n",
    "\n",
    "# NOTE: plotting the density to compare the distributions (meaningful 0 length and 1 length paths?)\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Path Lengths\")\n",
    "sns.histplot(\n",
    "    x=finished_paths.path_length[finished_paths.path_length < threshold],\n",
    "    ax=ax,\n",
    "    discrete=True,\n",
    "    alpha=0.4,\n",
    "    label=\"finished paths\",\n",
    "    stat=\"density\",\n",
    "    color=default_colors[0],\n",
    ")\n",
    "sns.histplot(\n",
    "    x=unfinished_paths.path_length[unfinished_paths.path_length < threshold],\n",
    "    ax=ax,\n",
    "    discrete=True,\n",
    "    alpha=0.4,\n",
    "    label=\"unfinished paths\",\n",
    "    stat=\"density\",\n",
    "    color=default_colors[1],\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of path lengths across finished, restarted paths and unfinished paths that timed out\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "sns.histplot(\n",
    "    x=finished_paths.path_length[finished_paths.path_length < threshold],\n",
    "    ax=axes[0],\n",
    "    discrete=True,\n",
    "    color=default_colors[0],\n",
    ")\n",
    "axes[0].set_title(\"Finished Paths\")\n",
    "axes[0].set_xlabel(\"Path Length\")\n",
    "\n",
    "unfinished_restart = unfinished_paths[\n",
    "    (unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"restart\")\n",
    "]\n",
    "sns.histplot(\n",
    "    data=unfinished_restart,\n",
    "    x=\"path_length\",\n",
    "    ax=axes[1],\n",
    "    discrete=True,\n",
    "    color=default_colors[0],\n",
    ")\n",
    "axes[1].set_title(\"Unfinished Paths - Restart\")\n",
    "axes[1].set_xlabel(\"Path Length\")\n",
    "\n",
    "unfinished_timeout = unfinished_paths[\n",
    "    (unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"timeout\")\n",
    "]\n",
    "sns.histplot(\n",
    "    data=unfinished_timeout,\n",
    "    x=\"path_length\",\n",
    "    ax=axes[2],\n",
    "    discrete=True,\n",
    "    color=default_colors[0],\n",
    ")\n",
    "axes[2].set_title(\"Unfinished Paths - Timeout\")\n",
    "axes[2].set_xlabel(\"Path Length\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Important note: unfinished paths before 2011 are missing\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "filter = lambda df, col: df[col]\n",
    "sns.histplot(\n",
    "    x=filter(finished_paths, \"datetime\"),\n",
    "    ax=ax,\n",
    "    alpha=0.4,\n",
    "    label=\"finished\",\n",
    "    bins=100,\n",
    "    color=default_colors[0],\n",
    ")\n",
    "sns.histplot(\n",
    "    x=filter(unfinished_paths, \"datetime\"),\n",
    "    ax=ax,\n",
    "    alpha=0.4,\n",
    "    label=\"unfinished\",\n",
    "    bins=50,\n",
    "    color=default_colors[1],\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfinished paths before 2011 are not present in the dataset. It is crucial to keep this in mind for subsequent analyses on differences between finished and unfinished paths, since in some cases the results could change if we include all the paths or we exclude those before 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing time (duration in seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correct the duration in seconds\n",
    "# Since the timeout is after 1800 seconds of inactivity, we consider here the correct duration\n",
    "unfinished_timeout.loc[:, \"effectiveDuration\"] = unfinished_timeout[\"durationInSec\"] - 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the distributions of game duration\n",
    "# different distribution for finished, restart and timeout\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "filter = lambda df, col: df[(df[col] >= 0) & (df[col] < 1000)][\n",
    "    col\n",
    "]  # chose 1000 for a better visualization\n",
    "sns.kdeplot(\n",
    "    x=filter(finished_paths, \"durationInSec\"),\n",
    "    ax=ax,\n",
    "    alpha=0.2,\n",
    "    linewidth=2,\n",
    "    fill=True,\n",
    "    label=\"finished\",\n",
    "    color=default_colors[0],\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=filter(unfinished_restart[unfinished_restart.path_length > 1], \"durationInSec\"),\n",
    "    ax=ax,\n",
    "    alpha=0.2,\n",
    "    linewidth=2,\n",
    "    fill=True,\n",
    "    label=\"restart\",\n",
    "    color=default_colors[1],\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=filter(\n",
    "        unfinished_timeout[unfinished_timeout.path_length > 1], \"effectiveDuration\"\n",
    "    ),\n",
    "    ax=ax,\n",
    "    alpha=0.2,\n",
    "    linewidth=2,\n",
    "    fill=True,\n",
    "    label=\"timeout\",\n",
    "    color=default_colors[2],\n",
    ")\n",
    "ax.set_xlim(left=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that we have some inconsistent values for `durationInSec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"{} paths are labeled as 'timeout' but their duration is below 1800 seconds (threshold for timeout)\".format(\n",
    "        len(\n",
    "            unfinished_timeout[\"durationInSec\"][\n",
    "                unfinished_timeout[\"durationInSec\"] < 1800\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of target articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note also that there are few articles that are very often chosen as targets (or as start articles). \n",
    "This could also influence further analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print articles' frequencies\n",
    "# This is computed with respect to all (finished and unfinished) paths\n",
    "target_counts = (\n",
    "    unfinished_restart[\"target\"].value_counts()\n",
    "    + finished_paths[\"target\"].value_counts()\n",
    "    + unfinished_timeout[\"target\"].value_counts()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "target_percentages = target_counts / target_counts.sum()\n",
    "\n",
    "print(\"The 10 most frequent targets (frequency)\")\n",
    "display(target_percentages.head(10))\n",
    "\n",
    "print(\"The 10 least frequent targets (frequency)\")\n",
    "display(target_percentages.sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print frequency of articles appearing as start articles\n",
    "# This is computed with respect to all (finished and unfinished) paths\n",
    "start_counts = (\n",
    "    unfinished_restart[\"start\"].value_counts()\n",
    "    + finished_paths[\"start\"].value_counts()\n",
    "    + unfinished_timeout[\"start\"].value_counts()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "start_percentages = start_counts / start_counts.sum()\n",
    "\n",
    "print(\"The 10 most frequent targets (frequency)\")\n",
    "display(start_percentages.head(10))\n",
    "\n",
    "print(\"The 10 least frequent targets (frequency)\")\n",
    "display(start_percentages.sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "\n",
    "## 2 - Data Analysis: Addressing the research subquestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cats\"></a>\n",
    "\n",
    "### 2.1 - Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the occurances of categories in the paths, to gain an understanding of whether certain categories lead to games that are on average easier for people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Categories in Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing which categories are most represented in articles\n",
    "count_articles = categories.groupby(\"broad_category\").size()\n",
    "\n",
    "print(\"Below shows how many articles each of the broad categories are represented by\")\n",
    "display(count_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for easy discovery of what categories an article belongs to\n",
    "article_to_category, article_to_broad_category = create_category_dictionaries(\n",
    "    categories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each category has occured as a target in the finished and unfinished paths.\n",
    "# Note that some articles are represented by multiple categories, which are thus counted extra.\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories of the target articles in the finished paths\n",
    "sorted_cats_f = sorted_category_counts(finished_paths, article_to_broad_category)\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories of the target articles in the unfinished paths\n",
    "sorted_cats_u = sorted_category_counts(unfinished_paths, article_to_broad_category)\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(\n",
    "    list(sorted_cats_f.keys()),\n",
    "    sorted_cats_f.values(),\n",
    "    label=\"Finished paths\",\n",
    "    color=default_colors[0],\n",
    ")\n",
    "ax2 = plt.barh(\n",
    "    list(sorted_cats_u.keys()),\n",
    "    sorted_cats_u.values(),\n",
    "    left=list(sorted_cats_f.values()),\n",
    "    label=\"Unfinished paths\",\n",
    "    color=default_colors[1],\n",
    ")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.title(\"Occurences of categories as targets\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows clearly that some categories occur as targets relatively more in finished paths while others have the opposite outcome. For example, \"Couuntries\" occurs as a target in finished paths multiple times as often as it does in unfinished paths, whereas \"Everyday_life\" occurs as a target in finished paths only slightly more often than it does in unfinished paths. \n",
    "\n",
    "The plot, however, also shows the imbalance in the categories. We have a lot of paths ending in \"Geography\" and \"Science\", but very few ending in \"Mathematics\" and \"Art\". We can create a plot with better interpretability regarding the influence of categories by charting the empirical likelihood of a target belonging to a certain category not being reached. This is the probability of a game being unfinished ($u$) for a given category $i$, and is calculated as:\n",
    "\n",
    "$\\Bbb{P}(u|i) = \\frac{\\text{num category i in unfinished paths target}}{\\text{num category i in target}}$\n",
    "\n",
    "However, a large part of the imbalance in the number of finished and unfinished paths comes from the fact that data on unfinished paths started to be collected only a couple of years after it did for finished paths. Thus, to have a probability value that makes sense, we need to exclude the finished path data from the time before data for unfinished paths also began to be collected. We will only perform this filtering for this particular analysis, however, as it results in a significant loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of counts of the broad categories of the targets in the finished paths played \n",
    "# after data began to be collected for unfinished paths.\n",
    "\n",
    "finished_paths_post2011 = finished_paths[\n",
    "    finished_paths[\"datetime\"] >= unfinished_paths[\"datetime\"].min()\n",
    "]\n",
    "sorted_cats_f_p2011 = sorted_category_counts(\n",
    "    finished_paths_post2011, article_to_broad_category\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the empirical likelihood that a target belonging to a certain category not being reached.\n",
    "cat_unfinished_prob = {}\n",
    "cat_unfinished_prob_CI = {}\n",
    "for cat in sorted_cats_f:\n",
    "    cat_unfinished_prob[cat] = sorted_cats_u[cat] / (\n",
    "        sorted_cats_u[cat] + sorted_cats_f_p2011[cat]\n",
    "    )\n",
    "    # Below gives the 95% confidence interval of the estimate using bootstrapping.\n",
    "    cat_unfinished_prob_CI[cat] = bootstrap_CI_prob_cat(\n",
    "        finished_paths_post2011[\"target\"], unfinished_paths[\"target\"], cat, article_to_broad_category, iterations=100\n",
    "        )\n",
    "    \n",
    "# Calculating the error bars from the confidence intervals.\n",
    "cat_err = [np.abs(l-list(cat_unfinished_prob.values())) for l in np.array(list(cat_unfinished_prob_CI.values())).T]\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(\n",
    "    list(cat_unfinished_prob.keys()),\n",
    "    cat_unfinished_prob.values(),\n",
    "    color=default_colors[0],\n",
    "    xerr=cat_err,\n",
    ")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Empirical likelihood of not reaching certain categories\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reinforces the possibility that certain categories make for easier games, as we can see that people are empirically more likely to not reach targets belonging to certain categories more than others.. Nonetheless, to establish a proper relationship, we may in the future need to control for certain other variables. For example, it could be that articles in the \"Countries\" category are simply better connected than those in the \"Everyday_life\" category. For this, we would need to potentially make use of matching and propensity scores to conduct a proper causal analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to explore the occurances of categories in the sources of the paths. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each category has occured as a source in the finished and unfinished paths.\n",
    "# Note that some articles are represented by multiple categories, which are thus counted extra.\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories of the source articles in the finished paths\n",
    "sorted_cats_source_f = sorted_category_counts(finished_paths, article_to_broad_category, part=\"start\")\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories of the source articles in the unfinished paths\n",
    "sorted_cats_source_u = sorted_category_counts(unfinished_paths, article_to_broad_category, part=\"start\")\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(\n",
    "    list(sorted_cats_source_f.keys()),\n",
    "    sorted_cats_source_f.values(),\n",
    "    label=\"Finished paths\",\n",
    "    color=default_colors[0],\n",
    ")\n",
    "ax2 = plt.barh(\n",
    "    list(sorted_cats_source_u.keys()),\n",
    "    sorted_cats_source_u.values(),\n",
    "    left=list(sorted_cats_source_f.values()),\n",
    "    label=\"Unfinished paths\",\n",
    "    color=default_colors[1],\n",
    ")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.title(\"Occurences of categories as starting articles\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart highlights how certain categories occur much more often as starting articles than others. Otherwise, it is difficult to discern which categories of starting articles lead to more unfinished paths. \n",
    "\n",
    "The same probability analysis done for the categories of the targets is thus conducted for the categories of the sources. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of counts of the broad categories of the sources in the finished paths played \n",
    "# after data began to be collected for unfinished paths.\n",
    "\n",
    "sorted_cats_source_f_p2011 = sorted_category_counts(\n",
    "    finished_paths_post2011, article_to_broad_category, part=\"start\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the empirical likelihood that a source belonging to a certain category leads to an unfinished game.\n",
    "cat_unfinished_source_prob = {}\n",
    "cat_unfinished_source_prob_CI = {}\n",
    "for cat in sorted_cats_source_f:\n",
    "    cat_unfinished_source_prob[cat] = sorted_cats_source_u[cat] / (\n",
    "        sorted_cats_source_u[cat] + sorted_cats_source_f_p2011[cat]\n",
    "    )\n",
    "    # Below gives the 95% confidence interval of the estimate using bootstrapping.\n",
    "    cat_unfinished_source_prob_CI[cat] = bootstrap_CI_prob_cat(\n",
    "        finished_paths_post2011[\"start\"], unfinished_paths[\"start\"], cat, article_to_broad_category, iterations=100\n",
    "        )\n",
    "    \n",
    "# Calculating the error bars from the confidence intervals.\n",
    "cat_source_err = [np.abs(l-list(cat_unfinished_source_prob.values())) for l in np.array(list(cat_unfinished_source_prob_CI.values())).T]\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(\n",
    "    list(cat_unfinished_source_prob.keys()),\n",
    "    cat_unfinished_source_prob.values(),\n",
    "    color=default_colors[0],\n",
    "    xerr=cat_source_err,\n",
    ")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Empirical likelihood of not finishing a game when starting from certain categories\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above highlights how the impact of the category of the starting article is not as important as that of the target article. Nonetheless, there are still some categories that stand out. Interestingly, Science and Everyday Life, which were among the harder categories to reach according to the previous analysis, are the two nicest categories to start out from. It may be possible that articles of these categories, while hard to reach, have links pointing out from them that reach a variety of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sub\"></a>\n",
    "\n",
    "#### 2.1.2 - Exploring Subject Strength in Articles\n",
    "\n",
    "Conducting an exploratory analysis on the relationship between categories of articles, our focus encompasses both neighboring articles (i.e., those directly connected by a link) and start/target articles in both finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 6 articles without category (found in section 1.2)!\n",
    "articles_categories = articles_categories.dropna(subset=[\"broad_category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat\"></a>\n",
    "\n",
    "##### Exploring Subject Strength in Connected Articles\n",
    "Visualizing the strength of the categories for connected articles (those which are connected by an edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the edges dataframe\n",
    "edge_category = merge_articles_categories(edges, [\"start\", \"end\"], articles_categories)\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "subject_graph, subject_edge_weights = visualize_article_connections_per_category(\n",
    "    edge_category,\n",
    "    articles_categories,\n",
    "    \"Article Connections Based on Category (Normalized and Scaled Edges)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_fi\"></a>\n",
    "\n",
    "##### Exploring Subject Strength in Finished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the finished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the finished_paths dataframe\n",
    "finished_paths_categories = merge_articles_categories(\n",
    "    finished_paths, [\"start\", \"target\"], articles_categories\n",
    ")\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "finished_graph, finished_edge_widths = visualize_article_connections_per_category(\n",
    "    finished_paths_categories,\n",
    "    articles_categories,\n",
    "    \"Start & Target Article Connections in Finished Path Based on Category (Normalized and Scaled Edges)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_unfi\"></a>\n",
    "\n",
    "##### Exploring Subject Strength in Uninished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the unfinished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the unfinished_paths dataframe\n",
    "unfinished_paths_categories = merge_articles_categories(\n",
    "    unfinished_paths, [\"start\", \"target\"], articles_categories\n",
    ")\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "unfinished_graph, unfinished_edge_widths = visualize_article_connections_per_category(\n",
    "    unfinished_paths_categories,\n",
    "    articles_categories,\n",
    "    \"Start & Target Article Connections in Unfinished Path Based on Category (Normalized and Scaled Edges)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet\"></a>\n",
    "\n",
    "### 2.2 - Article Metrics\n",
    "\n",
    "Conducting an analysis on the article metrics extracted in section 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet_cat\"></a>\n",
    "\n",
    "##### 2.2.1 - Analysing Article Metrics by Category\n",
    "\n",
    "Viewing how the different metrics (e.g., word_count, stopword_count, etc.) vary across different categories of articles. These differences are evident by visualizing the metrics in both bar plots and violin plots (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge articles with their corresponding categories\n",
    "article_metrics_with_categories = article_metrics.merge(\n",
    "    categories, how=\"left\", on=[\"article\"]\n",
    ")\n",
    "display(article_metrics_with_categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\n",
    "    \"word_count\",\n",
    "    \"stopword_count\",\n",
    "    \"stopword_percentage\",\n",
    "    \"non_stopword_count\",\n",
    "    \"non_stopword_percentage\",\n",
    "    \"avg_word_length\",\n",
    "    \"avg_sent_length\",\n",
    "    \"paragraph_count\",\n",
    "    \"readability_score\",\n",
    "]\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot))\n",
    ")\n",
    "\n",
    "article_metrics_with_categories[\"stopword_percentage\"] = 100 * article_metrics_with_categories[\"stopword_percentage\"]\n",
    "article_metrics_with_categories[\"non_stopword_percentage\"] = 100 * article_metrics_with_categories[\"non_stopword_percentage\"]\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    # Bar plot\n",
    "    ax_bar = axes[idx, 0]\n",
    "    sns.barplot(\n",
    "        x=article_metrics_with_categories[\"broad_category\"],\n",
    "        y=article_metrics_with_categories[metric],\n",
    "        errorbar=(\"ci\", 95),\n",
    "        ax=ax_bar,\n",
    "        palette=default_colors,\n",
    "    )\n",
    "    ax_bar.set_xlabel(\"Category\")\n",
    "    ax_bar.set_ylabel(metric)\n",
    "    ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "    ax_bar.set_xticklabels(ax_bar.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # Violin plot\n",
    "    ax_violin = axes[idx, 1]\n",
    "    sns.violinplot(\n",
    "        x=article_metrics_with_categories[\"broad_category\"],\n",
    "        y=article_metrics_with_categories[metric],\n",
    "        ax=ax_violin,\n",
    "        palette=default_colors,\n",
    "    )\n",
    "    ax_violin.set_xlabel(\"Category\")\n",
    "    ax_violin.set_ylabel(metric)\n",
    "    ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "    ax_violin.set_xticklabels(ax_violin.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* Interesting observations from the plot reveal that science articles generally tend to be shorter in length, evident in lower word counts and paragraph counts.\n",
    "* Mathematical article appear to be the least consistent in word length, with a notable high variance.\n",
    "* Articles related to countries, on average, display the lowest proportion of stopwords (percentage-wise), suggesting they might posses a rich vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmetfu_path\"></a>\n",
    "\n",
    "##### 2.2.2 - Analysing Article Metrics in Finished vs Unfinished paths\n",
    "\n",
    "Viewing how the different metrics (e.g., word_count, stopword_count, etc.) vary across start/target articles in finished/unfinished paths. Both bar plots and violin plots are used to understand the distribution our these metrics. We observe remarkable similarity accross all plots, but minor differences are apparent in word count and paragraph count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the article metrics per finished and unfinished parths (both for start and end articles)\n",
    "start_finished_article_metrics = finished_paths.merge(\n",
    "    article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\"\n",
    ")\n",
    "end_finished_article_metrics = finished_paths.merge(\n",
    "    article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\"\n",
    ")\n",
    "start_unfinished_article_metrics = unfinished_paths.merge(\n",
    "    article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\"\n",
    ")\n",
    "end_unfinished_article_metrics = unfinished_paths.merge(\n",
    "    article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\n",
    "    \"word_count\",\n",
    "    \"stopword_count\",\n",
    "    \"stopword_percentage\",\n",
    "    \"non_stopword_count\",\n",
    "    \"non_stopword_percentage\",\n",
    "    \"avg_word_length\",\n",
    "    \"avg_sent_length\",\n",
    "    \"paragraph_count\",\n",
    "    \"readability_score\",\n",
    "]\n",
    "dataframes = [\n",
    "    start_finished_article_metrics,\n",
    "    start_unfinished_article_metrics,\n",
    "    end_finished_article_metrics,\n",
    "    end_unfinished_article_metrics,\n",
    "]\n",
    "dataframe_labels = [\n",
    "    \"Start Finished\",\n",
    "    \"Start Unfinished\",\n",
    "    \"Target Finished\",\n",
    "    \"Target Unfinished\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot))\n",
    ")\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    data = [df[metric] for df in dataframes]\n",
    "\n",
    "    # Bar plot\n",
    "    ax_bar = axes[idx, 0]\n",
    "    sns.barplot(data=data, errorbar=(\"ci\", 95), ax=ax_bar, palette=default_colors)\n",
    "    ax_bar.set_xlabel(\"Type of article\")\n",
    "    ax_bar.set_ylabel(metric)\n",
    "    ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "    ax_bar.set_xticklabels(dataframe_labels)\n",
    "\n",
    "    # Violin plot\n",
    "    ax_violin = axes[idx, 1]\n",
    "    sns.violinplot(data=data, ax=ax_violin, palette=default_colors)\n",
    "    ax_bar.set_xlabel(\"Type of article\")\n",
    "    ax_violin.set_ylabel(metric)\n",
    "    ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "    ax_violin.set_xticklabels(dataframe_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing article metric differences in starting articles in finished/unfinished paths.\n",
    "print(\"Start Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(\n",
    "    metrics_to_plot, start_finished_article_metrics, start_unfinished_article_metrics\n",
    ")\n",
    "\n",
    "# Comparing article metric differences in target articles in finished/unfinished paths.\n",
    "print(\"\\nTarget Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(\n",
    "    metrics_to_plot, end_finished_article_metrics, end_unfinished_article_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# Comparing article metric differences in (start, target) article pairs in finished/unfinished paths.\n",
    "# This enables us to identify differences between the articles one starts with and those that need to be finished\n",
    "# in both finished/unfinished paths, aiming to uncover potential factors that might impact the player.\n",
    "print(\"\\nFinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(\n",
    "    metrics_to_plot, start_finished_article_metrics, end_finished_article_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nUnfinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(\n",
    "    metrics_to_plot, start_unfinished_article_metrics, end_unfinished_article_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics                    | Start Articles (Finished vs Unfinished) | Target Articles (Finished vs Unfinished) | Finished Articles (Start vs Target) | Unfinished Articles (Start vs Target) |\n",
    "|----------------------------|-----------------------------------------|------------------------------------------|--------------------------------------|----------------------------------------|\n",
    "| word_count                  | t-statistic: 1.873, p-value: 0.061       | t-statistic: 36.838, p-value: 0.000      | t-statistic: -30.824, p-value: 0.000 | t-statistic: 8.949, p-value: 0.000     |\n",
    "| stopword_count              | t-statistic: 1.353, p-value: 0.176       | t-statistic: 34.692, p-value: 0.000      | t-statistic: -30.009, p-value: 0.000 | t-statistic: 8.076, p-value: 0.000     |\n",
    "| stopword_percentage         | t-statistic: -3.366, p-value: 0.001      | t-statistic: 4.045, p-value: 0.000       | t-statistic: -0.362, p-value: 0.717 | t-statistic: 6.045, p-value: 0.000     |\n",
    "| non_stopword_count          | t-statistic: 2.131, p-value: 0.033       | t-statistic: 37.599, p-value: 0.000      | t-statistic: -30.943, p-value: 0.000 | t-statistic: 9.317, p-value: 0.000     |\n",
    "| non_stopword_percentage     | t-statistic: 3.366, p-value: 0.001       | t-statistic: -4.045, p-value: 0.000      | t-statistic: 0.362, p-value: 0.717  | t-statistic: -6.045, p-value: 0.000    |\n",
    "| avg_word_length             | t-statistic: -3.090, p-value: 0.002      | t-statistic: 10.974, p-value: 0.000     | t-statistic: 2.987, p-value: 0.003  | t-statistic: 14.113, p-value: 0.000   |\n",
    "| avg_sent_length             | t-statistic: 4.863, p-value: 0.000       | t-statistic: -0.260, p-value: 0.795     | t-statistic: -3.443, p-value: 0.001 | t-statistic: -6.964, p-value: 0.000    |\n",
    "| paragraph_count             | t-statistic: 0.038, p-value: 0.970       | t-statistic: 37.247, p-value: 0.000     | t-statistic: -29.294, p-value: 0.000| t-statistic: 11.952, p-value: 0.000   |\n",
    "| readability_score           | t-statistic: 4.652, p-value: 0.000       | t-statistic: -21.100, p-value: 0.000   | t-statistic: -8.953, p-value: 0.000 | t-statistic: -27.779, p-value: 0.000 |\n",
    "\n",
    "1. **Finished vs Unfinished Start Articles:**\n",
    "   - The stopword_percentage is significantly lower (and non_stopword_percentage higher) in finished articles than unfinished, suggesting a potential emphasis on more meaningful content. \n",
    "   - Finished start articles also tend to have higher avg_sent_length and readability_score, indicating a focus on well-structured and reader-friendly content.\n",
    "\n",
    "2. **Finished vs Unfinished Target Articles:**\n",
    "   - Finished target articles exhibit significantly higher values across various metrics, including word_count, stopword_percentage (with lower non_stopword_percentage), avg_word_length, and paragraph_count. \n",
    "   - Additionally, they have a significantly lower readability_score, suggesting that finished target articles could be more challenging to comprehend.\n",
    "\n",
    "3. **Finished Articles (Start, Target):**\n",
    "   - When comparing finished start and target articles, strong differences emerge in various metrics. \n",
    "   - Finished start articles tend to be shorter with lower word_count and paragraph counts, but have a slightly higher readability_score.\n",
    "\n",
    "4. **Unfinished Articles (Start, Target):**\n",
    "   - Starting articles exhibit significantly higher values accross all metrics, except avg_sent_length, non_stopword_percentage, and readability_score. \n",
    "   - This implies that target articles which players failed to complete, are consistently longer, include more stopword_percentage (therefore have a lower non_stopword_percentage) and have higher avg_word_length. \n",
    "   - Interestingly, unfinished start/target articles exhibit the most significant difference in readability scores, prompting us to consider whether this larger gap affects the user's ability to finish the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"difficulty\"></a>\n",
    "\n",
    "#### 2.3 - Path difficulty\n",
    "\n",
    "Conducting an analysis on the objective difficulty measure per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ltt\"></a>\n",
    "\n",
    "##### 2.3.1 - Analysing the In-Degree of Targets in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that certain paths are easier objectively because their targets have a larger \"in-degree\", i.e. the number of edges in the graph pointing to it. This would be intuitive: if there are more ways to get to the target, it should be easier to do so. This section explores whether this idea is reflected in the distributions of the in-degrees of the targets in finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many links point to targets in finished and unfinished paths, known as the \"in-degree\".\n",
    "\n",
    "finished_paths[\"links_to_target\"] = finished_paths[\"path\"].apply(\n",
    "    lambda x: len(edges.loc[edges[\"end\"] == x[-1]])\n",
    ")\n",
    "unfinished_paths[\"links_to_target\"] = unfinished_paths[\"target\"].apply(\n",
    "    lambda x: len(edges.loc[edges[\"end\"] == x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for targets with an in-degree of 0.\n",
    "\n",
    "zeros_finished = [1 for x in finished_paths[\"links_to_target\"] if x == 0]\n",
    "zeros_unfinished = [1 for x in unfinished_paths[\"links_to_target\"] if x == 0]\n",
    "\n",
    "print(\n",
    "    \"There were {} targets with an in-degree of 0 in the finished paths.\".format(\n",
    "        sum(zeros_finished)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"There were {} targets with an in-degree of 0 in the unfinished paths.\".format(\n",
    "        sum(zeros_unfinished)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unreasonable that the targets have an in-degree of 0. It is likely that these outcomes occur because the information provided in the data was not updated after the wikipedia graph kept evolving, as one target with 0 in-degree was reached. It is nonetheless believed that the information is accurate in the majority of cases, so we simply ignore the targets with 0 in-degree in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the data below.\n",
    "\n",
    "finished_in_degree = [x for x in finished_paths[\"links_to_target\"] if x != 0]\n",
    "unfinished_in_degree = [x for x in unfinished_paths[\"links_to_target\"] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(finished_in_degree, bins=50)\n",
    "ax1.set_xlabel(\"in-degree\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of in-degree of targets in finished paths\")\n",
    "\n",
    "ax2.hist(unfinished_in_degree, bins=50)\n",
    "ax2.set_xlabel(\"in-degree\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of in-degree of targets in unfinished paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show that the data is clearly heavy tailed. We suspect this may be a power law. We check this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the arrays of for the cumulative distributions of in-degrees:\n",
    "finished_indegree_cumulative = plt.hist(\n",
    "    finished_paths.links_to_target,\n",
    "    bins=100,\n",
    "    log=True,\n",
    "    cumulative=-1,\n",
    "    histtype=\"step\",\n",
    "    color=default_colors[0],\n",
    ")\n",
    "unfinished_indegree_cumulative = plt.hist(\n",
    "    unfinished_paths.links_to_target,\n",
    "    bins=100,\n",
    "    log=True,\n",
    "    cumulative=-1,\n",
    "    histtype=\"step\",\n",
    "    color=default_colors[1],\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "# Plotting the CCDF plots of the in-degrees for finished and unfinished paths:\n",
    "plt.loglog(\n",
    "    finished_indegree_cumulative[1][1:],\n",
    "    finished_indegree_cumulative[0],\n",
    "    label=\"Finished paths\",\n",
    ")\n",
    "plt.loglog(\n",
    "    unfinished_indegree_cumulative[1][1:],\n",
    "    unfinished_indegree_cumulative[0],\n",
    "    label=\"Unfinished paths\",\n",
    ")\n",
    "plt.title(\"CCDF plot of the in-degree of targets\")\n",
    "plt.ylabel(\"# of targets (in log scale)\")\n",
    "plt.xlabel(\"In-degree (in log scale)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that the in-degree may not necessarily follow a power law, as the CCDF plot does not have a negative linear slope. We thus try to see if the data is log-normal, be plotting the histogram of the logarithms of the in-degrees of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the logs of the data below.\n",
    "\n",
    "logged_finished = [np.log(x) for x in finished_paths[\"links_to_target\"] if x != 0]\n",
    "logged_unfinished = [np.log(x) for x in unfinished_paths[\"links_to_target\"] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(logged_finished, bins=8, color=default_colors[0])\n",
    "ax1.set_xlabel(\"log(in-degree)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of in-degree of targets in finished paths\")\n",
    "\n",
    "ax2.hist(logged_unfinished, bins=8, color=default_colors[0])\n",
    "ax2.set_xlabel(\"log(in-degree)\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of in-degree of targets in unfinished paths\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show that the data may indeed be distributed log-normally. This is why we report both the mean and the median values for these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean in-degree of the targets in the finished and unfinished paths.\n",
    "print(\n",
    "    \"The targets that were reached had an in-degree of {:.3f} on average.\".format(\n",
    "        mean(finished_in_degree)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The targets that were not reached had an in-degree of {:.3f} on average.\".format(\n",
    "        mean(unfinished_in_degree)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing median in-degree of the targets in the finished and unfinished paths.\n",
    "print(\n",
    "    \"The targets that were reached had a median in-degree of {}.\".format(\n",
    "        median(finished_in_degree)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The targets that were not reached had a median in-degree of {}.\".format(\n",
    "        median(unfinished_in_degree)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a t-test assums a normal distribution around the mean, we cannot do the t-test for the in-degrees directly, which are heavily skewed. Instead, we do it for the logarithms of the in-degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting a t-test for the in-degree of targets for finished and unfinished paths.\n",
    "simple_t_test(logged_finished, logged_unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the number of links pointing to the targets of finished and unfinished paths is 0.0. This means we reject the null hypothesis that the number of links pointing to the targets are statistically the same at the 5% level of significance, indicating that the in-degree of the target indeed may have a statistical significance in whether a game will be finished or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a box plot of the trends.\n",
    "\n",
    "finished_links = pd.DataFrame()\n",
    "finished_links[\"links_to_target\"] = finished_in_degree\n",
    "finished_links[\"path_type\"] = \"Finished paths\"\n",
    "\n",
    "unfinished_links = pd.DataFrame()\n",
    "unfinished_links[\"links_to_target\"] = unfinished_in_degree\n",
    "unfinished_links[\"path_type\"] = \"Unfinished paths\"\n",
    "\n",
    "df_links = pd.concat([finished_links, unfinished_links])\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"path_type\", y=\"links_to_target\", data=df_links, palette=default_colors\n",
    ")\n",
    "plt.xlabel(\" \")\n",
    "plt.ylim([-5, 200])\n",
    "plt.ylabel(\"In-degree of target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above highlight these conclusions. The in-degree of targets in the finished paths are noticeably higher than those in the unfinished paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lss\"></a>\n",
    "\n",
    "##### 2.3.2 - Analysing the Out-Degree of Sources in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from the previous analysis, it is also possible that certain paths are easier objectively because their starting articles have a larger \"out-degree\", i.e. the number of edges in the graph pointing ot from it. This would be intuitive: if there are many directions to go from the source, it should be easier to reach the target. This section explores whether this idea is reflected in the distributions of the out-degrees of the sources in finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many links point from sources in finished and unfinished paths, known as the \"out-degree\".\n",
    "\n",
    "finished_paths[\"links_from_source\"] = finished_paths[\"start\"].apply(\n",
    "    lambda x: len(edges.loc[edges[\"start\"] == x])\n",
    ")\n",
    "unfinished_paths[\"links_from_source\"] = unfinished_paths[\"start\"].apply(\n",
    "    lambda x: len(edges.loc[edges[\"start\"] == x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for sources with an out-degree of 0.\n",
    "\n",
    "zeros_source_finished = [1 for x in finished_paths[\"links_from_source\"] if x == 0]\n",
    "zeros_source_unfinished = [1 for x in unfinished_paths[\"links_from_source\"] if x == 0]\n",
    "\n",
    "print(\n",
    "    \"There were {} sources with an out-degree of 0 in the finished paths.\".format(\n",
    "        sum(zeros_source_finished)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"There were {} sources with an out-degree of 0 in the unfinished paths.\".format(\n",
    "        sum(zeros_source_unfinished)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unreasonable that the sources have an out-degree of 0. It is also likely that these outcomes occur because the information provided in the data was not updated after the wikipedia graph kept evolving, as it would be impossible to have a path if the starting article cannot lead anywhere. It is nonetheless believed that the information is accurate in the majority of cases, so we simply ignore the targets with 0 out-degree in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the data below.\n",
    "\n",
    "finished_out_degree = [x for x in finished_paths[\"links_from_source\"] if x != 0]\n",
    "unfinished_out_degree = [x for x in unfinished_paths[\"links_from_source\"] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(finished_out_degree, bins=50)\n",
    "ax1.set_xlabel(\"out-degree\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of out-degree of sources in finished paths\")\n",
    "\n",
    "ax2.hist(unfinished_out_degree, bins=50)\n",
    "ax2.set_xlabel(\"out-degree\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of out-degree of sources in unfinished paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows again that the data is heavy-tailed. We suspect that the data may be log-normal, which we try to see by plotting the histogram of the logarithms of the out-degrees of the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the logs of the data of out-degrees below.\n",
    "\n",
    "logged_finished_source = [np.log(x) for x in finished_paths[\"links_from_source\"] if x != 0]\n",
    "logged_unfinished_source = [np.log(x) for x in unfinished_paths[\"links_from_source\"] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(logged_finished_source, bins=8, color=default_colors[0])\n",
    "ax1.set_xlabel(\"log(out-degree)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of out-degree of targets in finished paths\")\n",
    "\n",
    "ax2.hist(logged_unfinished_source, bins=8, color=default_colors[0])\n",
    "ax2.set_xlabel(\"log(out-degree)\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of out-degree of targets in unfinished paths\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show that the data may indeed be distributed log-normally. This is why we report both the mean and the median values for these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean out-degree of the sources in the finished and unfinished paths.\n",
    "print(\n",
    "    \"The finished games had a starting article with an out-degree of {:.3f} on average.\".format(\n",
    "        mean(finished_out_degree)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The unfinished games had a starting article with an out-degree of {:.3f} on average.\".format(\n",
    "        mean(unfinished_out_degree)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing median out-degree of the sources in the finished and unfinished paths.\n",
    "print(\n",
    "    \"The finished games had a starting article with a median out-degree of {:.3f}.\".format(\n",
    "        median(finished_out_degree)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The unfinished games had a starting article with a median out-degree of {:.3f}.\".format(\n",
    "        median(unfinished_out_degree)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a t-test assums a normal distribution around the mean, we cannot do the t-test for the out-degrees directly, which are heavily skewed. Instead, we do it for the logarithms of the out-degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting a t-test for the in-degree of targets for finished and unfinished paths.\n",
    "simple_t_test(logged_finished_source, logged_unfinished_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the number of links pointing froum the sources of finished and unfinished paths is 0.289. This means we cannot reject the null hypothesis that the number of links pointing from the sources are statistically the same at the 5% level of significance, indicating that the out-degree of the source may not in fact have a statistical significance in whether a game will be finished or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a box plot of the trends.\n",
    "\n",
    "finished_links_source = pd.DataFrame()\n",
    "finished_links_source[\"links_from_source\"] = finished_out_degree\n",
    "finished_links_source[\"path_type\"] = \"Finished paths\"\n",
    "\n",
    "unfinished_links_source = pd.DataFrame()\n",
    "unfinished_links_source[\"links_from_source\"] = unfinished_out_degree\n",
    "unfinished_links_source[\"path_type\"] = \"Unfinished paths\"\n",
    "\n",
    "df_links_source = pd.concat([finished_links_source, unfinished_links_source])\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    x=\"path_type\", y=\"links_from_source\", data=df_links_source, palette=default_colors\n",
    ")\n",
    "plt.xlabel(\" \")\n",
    "plt.ylim([-5, 100])\n",
    "plt.ylabel(\"Out-degree of source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above highlight these conclusions. The out-degree of sources in the finished paths seem almost the same as those in the unfinished paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shortest\"></a>\n",
    "\n",
    "##### 2.3.3 - Analysing Possible Shortest Path Distances in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential factor that may determine whether a game will be completed or not, in a more objective manner, is the shortest path length possible between the source and the target. This factor is also intuitive. If a shorter path exists in theory, the path length should also be shorter on average in practice, leading to simpler games. This section explores whether this idea is reflected in the distributions of the length of the shortest possible paths in finished and unfinished games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the finished games.\n",
    "\n",
    "finished_paths[\"shortest_path_length\"] = finished_paths[\"path\"].apply(\n",
    "    lambda x: shortest_paths[articles.loc[articles[\"article\"] == x[0]].index[0]][\n",
    "        articles.loc[articles[\"article\"] == x[-1]].index[0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: There are typos in the targets.\n",
    "\n",
    "Eg. At index 141 in unfinished paths, the target is written as \"Long_peper\", when it should be \"Long_pepper\".\n",
    "\n",
    "Overall, an issue arises in unfinished paths 28 times, but this doesn't seem to be an issue in finished paths. These data points are ignored so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the unfinished games.\n",
    "\n",
    "shortest_unfinished, not_found_unfinished = shortest_path_find(\n",
    "    unfinished_paths, articles, shortest_paths\n",
    ")\n",
    "\n",
    "unfinished_paths[\"shortest_path_length\"] = shortest_unfinished\n",
    "print(f\"{not_found_unfinished} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to confirm that there are no issues in the finished paths.\n",
    "\n",
    "shortest_finished, not_found_finished = shortest_path_find(\n",
    "    finished_paths, articles, shortest_paths\n",
    ")\n",
    "\n",
    "print(f\"{not_found_finished} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for \"impossible\" paths.\n",
    "\n",
    "print(\n",
    "    \"There is {} impossible finished path.\".format(\n",
    "        len(finished_paths[finished_paths[\"shortest_path_length\"] == 255])\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"There are {} impossible unfinished paths.\".format(\n",
    "        len(unfinished_paths[unfinished_paths[\"shortest_path_length\"] == 255])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the case where there were targets with an in-degree of 0, it is likely that \"impossible\" paths exist because the shortest paths matrix provided in the data was not updated after the wikipedia graph kept evolving, as one impossible path was completed. As we determine the shortest possible path length based on that matrix, its errors reflect on our analysis. It is nonetheless believed that the matrix is accurate in the majority of cases, so we simply ignore the \"impossible\" paths in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the data below.\n",
    "shortest_possible_finished = finished_paths[\n",
    "    finished_paths[\"shortest_path_length\"] != 255\n",
    "][\"shortest_path_length\"]\n",
    "shortest_possible_unfinished = unfinished_paths[\n",
    "    unfinished_paths[\"shortest_path_length\"] != 255\n",
    "][\"shortest_path_length\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(\n",
    "    shortest_possible_finished, bins=[1, 2, 3, 4, 5, 6, 7, 8], color=default_colors[0]\n",
    ")\n",
    "ax1.set_xlabel(\"Length of shortest possible path\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Shortest path length possible from source to target in finished paths\")\n",
    "\n",
    "ax2.hist(\n",
    "    shortest_possible_unfinished, bins=[1, 2, 3, 4, 5, 6, 7, 8], color=default_colors[0]\n",
    ")\n",
    "ax2.set_xlabel(\"Length of shortest possible path\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Shortest path length possible from source to target in unfinished paths\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of the shortest possible path length makes clear that unfinished paths tend to have longer possible shortest lengths. Intuitively, it makes sense that this would be the case.\n",
    "\n",
    "It also reveals how the shortest possible path length follows a fairly normal distribution, so we may continue with our analysis. The means of the distributions provide sufficient information in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean shortest possible paths in the finished and unfinished paths.\n",
    "\n",
    "print(\n",
    "    \"The shortest possible paths were {:.3f} long on average in the finished paths.\".format(\n",
    "        shortest_possible_finished.mean()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The shortest possible paths were {:.3f} long on average in the unfinished paths.\".format(\n",
    "        shortest_possible_unfinished.mean()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a t test on the shortest path lengths.\n",
    "simple_t_test(shortest_possible_finished, shortest_possible_unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the shortest possible path lengths of finished and unfinished games is 0.0. This means we reject the null hypothesis that the shortest possible game paths are statistically the same across the two groups at the 5% level of significance, and thus the length of the shortest path possible does indeed may have a statistically significant effect on whether a game will be completed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting situation. The past two analyses show that the targets are more difficult to get to in the unfinished paths, due to their lower in-degree and the larger value of the possible shortest path to them.\n",
    "\n",
    "A challenge for us may be to try to isolate whether the difference between whether a path is finished or not can be fully explained by more objective factors like this, or if there is a human component that we can isolate as well, when controlling for factors such as these. Eg, are some categories actually more difficult to get to, or do the differences in the target category distributions in the finished and unfinished paths arise because some categories may be more likely to have longer possible shortest paths to them or have fewer links pointing at them? These differences will be disentangled further below in the notebook with logistic regression and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"indiv_behviour\"></a>\n",
    "\n",
    "#### 2.4 - Individual Player Behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to explore the effect of the use of backclicks on the likelihood to complete the search game and reach the target page.<br>\n",
    "In particular, our hypothesis is that the **use of backclics may be an indicator of likely quitting**. It's easy to think that a backclick can mean the user ended up on a page different from what he/she thought, or maybe the backclicked page is missing the links he/she was thinking to found there. Anyway, let's dive into the analysis. \n",
    "\n",
    "NOTE: for the following analyses, we will only consider players who played multiple games, exclude all games shorter than a certain threshold and consider only unfinished games where the player actively quitted, so only the one of type \"restart\". In order to filter out less meaningful data and allows to group data from multiple games played by the same user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter games keeping only data collected from players who played at least 10 games, considering only paths with at least 5 clicks and only restarted games\n",
    "pers_finished_df, pers_unfinished_df = filter_games(\n",
    "    finished_paths, unfinished_paths, min_games=10, min_length=5, type=\"restart\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bck_distr\"></a>\n",
    "##### 2.4.1. Back-clicks frequency distribution\n",
    "How frequently do players back-click in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two filtered dataframes, adding a column to identify finished and unfinished paths\n",
    "bck_an_all = pd.concat(\n",
    "    [pers_finished_df.assign(finished=True), pers_unfinished_df.assign(finished=False)],\n",
    "    ignore_index=True,\n",
    ")[[\"hashedIpAddress\", \"path\", \"finished\"]]\n",
    "\n",
    "# Compute backclicked pages (pages a \"<\" correponds to) and visited pages (path without the \"<\")\n",
    "bck_an_all[\"backclicked_pages\"] = bck_an_all[\"path\"].apply(\n",
    "    lambda x: get_backclicked_pages(x)\n",
    ")\n",
    "bck_an_all[\"visited_pages\"] = bck_an_all[\"path\"].apply(\n",
    "    lambda x: [el for el in x if el != \"<\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    data=bck_an_all[\"backclicked_pages\"].apply(\n",
    "        lambda x: len(x) if x is not pd.NA else 0\n",
    "    ),\n",
    "    kde=True,\n",
    "    bins=50,\n",
    ")\n",
    "plt.title(\"Distribution of backclicked pages per game\")\n",
    "plt.xlabel(\"Number of backclicked pages\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the distribution is highly skewed since most of the games have 0 or nearly 0 backclicked pages, but there are a few games with both a lot of visited and backclicked pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the impact of the backclicks on the likelyhood of quitting the game, we will then consider the **backclick frequency** per game, defined as the number of backclicked pages divided by the number of visited pages in that path.<br>\n",
    "Let's visualize the back-click frequency distribution in two cases:\n",
    "- first considering all the games independently\n",
    "- then grouping and averaging back-click frequency of same player over multiple games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backclick frequency for each game as the number of backclicked pages divided by the number of visited pages\n",
    "bck_an_all[\"backclick_number\"] = bck_an_all[\"backclicked_pages\"].apply(lambda x: len(x))\n",
    "bck_an_all[\"visited_number\"] = bck_an_all[\"visited_pages\"].apply(lambda x: len(x))\n",
    "\n",
    "bck_an_all[\"backclick_freq\"] = (\n",
    "    bck_an_all[\"backclick_number\"] / bck_an_all[\"visited_number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "\n",
    "sns.histplot(data=bck_an_all[\"backclick_freq\"], kde=True, bins=30, ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of backclick frequency for all games\")\n",
    "axs[0].set_xlabel(\"back-click-rate\")\n",
    "axs[0].set_ylabel(\"Number of games\")\n",
    "\n",
    "sns.histplot(\n",
    "    data=bck_an_all[[\"hashedIpAddress\", \"backclick_freq\"]]\n",
    "    .groupby(\"hashedIpAddress\")\n",
    "    .mean()[\"backclick_freq\"],\n",
    "    kde=True,\n",
    "    bins=30,\n",
    "    ax=axs[1],\n",
    ")\n",
    "axs[1].set_title(\n",
    "    \"Distribution of avg backclick frequency for each player over all their games\"\n",
    ")\n",
    "axs[1].set_xlabel(\"Avg backclick-rate\")\n",
    "axs[1].set_ylabel(\"Number of players\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference between the two distributions suggests that the use of backclicks is a characteristic of the player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-freq-compare></a>\n",
    "#### 2.4.2 Back-click frequency distribution in finished vs unfinished paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the average backclick frequency distribution between finished and unfinished paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharey=True, sharex=True)\n",
    "\n",
    "sns.violinplot(\n",
    "    bck_an_all[bck_an_all[\"finished\"] == True]\n",
    "    .groupby(\"hashedIpAddress\")[\"backclick_freq\"]\n",
    "    .mean(),\n",
    "    ax=axs[0],\n",
    "    color=default_colors[0],\n",
    "    inner=\"quart\",\n",
    ")\n",
    "sns.violinplot(\n",
    "    bck_an_all[bck_an_all[\"finished\"] == False]\n",
    "    .groupby(\"hashedIpAddress\")[\"backclick_freq\"]\n",
    "    .mean(),\n",
    "    ax=axs[1],\n",
    "    color=default_colors[1],\n",
    "    inner=\"quart\",\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Distribution of avg backclick frequency for each player over all their games\"\n",
    ")\n",
    "axs[0].set_title(\"Finished games\")\n",
    "axs[1].set_title(\"Unfinished games\")\n",
    "axs[0].set_ylabel(\"Avg backclick-frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This violin plot shows how the average backclick frequency is often higher over unfinished paths, giving us the hint this parameter may be an indicator of a more likely quit of the player. <br>\n",
    "Still we'll need further analysis (with a statistically significant test) to draw some more precise conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-rate-cat></a>\n",
    "#### 2.4.3 Backclick rate per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do players backclick more frequently on some categories? Do things change if we consider only finished or unfinished paths?\n",
    "\n",
    "To start exploring possible answers to these questions, let's define the **back-click rate per category** $c$ for a player $P$ as the number of times that $P$ has baclicked on a page that belongs to the category $c$, divide by the number of times $P$ visited a page of that category. This measure should express how likely is the player $P$ to back-click a page of category $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to count the number of backclicks and visits for each category\n",
    "bck_an_all[\"visited_categories\"] = bck_an_all[\"visited_pages\"].apply(\n",
    "    lambda x: [\n",
    "        article_to_broad_category[page][0]\n",
    "        for page in x\n",
    "        if page in article_to_broad_category.keys()\n",
    "    ]\n",
    ")\n",
    "bck_an_all[\"backclicked_categories\"] = bck_an_all[\"backclicked_pages\"].apply(\n",
    "    lambda x: [\n",
    "        article_to_broad_category[page][0]\n",
    "        for page in x\n",
    "        if page in article_to_broad_category.keys()\n",
    "    ]\n",
    "    if x is not pd.NA\n",
    "    else pd.NA\n",
    ")\n",
    "\n",
    "# Merge the two dataframes to count visits and backclicks per category\n",
    "backclicks_per_category = pd.merge(\n",
    "    bck_an_all[[\"hashedIpAddress\", \"visited_categories\", \"finished\"]]\n",
    "    .explode(\"visited_categories\")\n",
    "    .groupby([\"hashedIpAddress\", \"visited_categories\", \"finished\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"visits_count\"),\n",
    "    bck_an_all[[\"hashedIpAddress\", \"backclicked_categories\", \"finished\"]]\n",
    "    .explode(\"backclicked_categories\")\n",
    "    .groupby([\"hashedIpAddress\", \"backclicked_categories\", \"finished\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"backclicks_count\"),\n",
    "    how=\"left\",\n",
    "    left_on=[\"hashedIpAddress\", \"visited_categories\", \"finished\"],\n",
    "    right_on=[\"hashedIpAddress\", \"backclicked_categories\", \"finished\"],\n",
    ")\n",
    "\n",
    "backclicks_per_category.drop(columns=[\"backclicked_categories\"], inplace=True)\n",
    "backclicks_per_category.rename(columns={\"visited_categories\": \"category\"}, inplace=True)\n",
    "backclicks_per_category[\"backclicks_count\"].fillna(0, inplace=True)\n",
    "backclicks_per_category[\"backclick_rate_per_category\"] = (\n",
    "    backclicks_per_category[\"backclicks_count\"]\n",
    "    / backclicks_per_category[\"visits_count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a paired-column bar chart, showing for each category the average backclick-rate for finished and unfinished games\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(\n",
    "    data=backclicks_per_category[\n",
    "        [\"category\", \"finished\", \"backclick_rate_per_category\"]\n",
    "    ],\n",
    "    x=\"category\",\n",
    "    y=\"backclick_rate_per_category\",\n",
    "    hue=\"finished\",\n",
    "    order=backclicks_per_category[backclicks_per_category[\"finished\"] == False][\n",
    "        [\"category\", \"backclick_rate_per_category\"]\n",
    "    ]\n",
    "    .groupby(\"category\")\n",
    "    .mean()\n",
    "    .sort_values(by=\"backclick_rate_per_category\", ascending=False)\n",
    "    .index,\n",
    "    hue_order=[True, False],\n",
    "    errorbar=(\"ci\", 95),\n",
    "    errwidth=1.4,\n",
    "    capsize=0.1,\n",
    "    estimator=np.mean,\n",
    ")\n",
    "plt.xticks(rotation=80)\n",
    "plt.title(\"Avg backclick-rate per category: finished vs unfinished games\")\n",
    "plt.ylabel(\"Avg backclick-rate per category\", labelpad=20)\n",
    "plt.xlabel(\"Category\", labelpad=20)\n",
    "plt.legend(title=\"Finished game\", ncols=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart shows that the average back-click rate is often significantly different for finished and unfinished paths.\n",
    "Since this data compare finished and unfinished per-category backclicl-rate of all players, we could see that it's likely that a player with a high back-click rate will quit, even if the measure of how likely is this case, depends on the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-test></a>\n",
    "#### 2.4.4 Backclick frequency impact on finishing the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to the starting point: is there a significant relationship between the backclick frequency over a game and the fact this game may have ended or not?\n",
    "\n",
    "A first answer can be found by looking to the [**point biserial correlation**](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient), which is a correlation coefficient, mathematically equivalent to the Pearson correlation, specific for cases where we have one continouous variable and a dichotomous one. <br>\n",
    "This setting fits our needs, in particular we can consider the following random variables:\n",
    "- $X=\\lbrace1 \\text{ if the game is finished, } 0\\text{ if the game is not finished}\\rbrace$ $\\implies$ dichotomous variable\n",
    "- $Y=\\text{backclick\\_frequency}$ $\\implies$ continuous variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider a simple case where all the different games are considered independently each other, so for this first analysis we won't group games played by the same user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(x=bck_an_all[\"finished\"], y=bck_an_all[\"backclick_freq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see a *small* negative correlation which is statistically significant. <br>\n",
    "In particular, the fact that the correlation is negative already tells us that the higher the $\\text{backclick\\_frequency}$ the lower the probability of finishing the game $\\Bbb{P}(X=1)$.\n",
    "\n",
    "Let's now group data from different games of the same player, in particular, we'll take for each player it's average $\\text{backclick\\_frequency}$ over his/her finished games and over his/her unfinished games. <br>\n",
    "By doing this we'll be able to tell whether there is a significant difference in the average backclick rate of each player, between his/her finished vs. unfinished games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(\n",
    "    x=bck_an_all.groupby([\"hashedIpAddress\", \"finished\"])[\"backclick_freq\"]\n",
    "    .mean()\n",
    "    .reset_index()[\"finished\"],\n",
    "    y=bck_an_all.groupby([\"hashedIpAddress\", \"finished\"])[\"backclick_freq\"]\n",
    "    .mean()\n",
    "    .reset_index()[\"backclick_freq\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the correlation is even higher (in absolute value) and still statistically significant, leading us to think that the backclick frequency of a player may be an indicator of likely quitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint1\"></a>\n",
    "\n",
    "## Checkpoint for dataframe\n",
    "To avoid running intensive computations again and again, we save the result in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code saves the version of the dataframe above:\n",
    "finished_paths.to_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"finished_paths_stats.pkl\")\n",
    ")\n",
    "unfinished_paths.to_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"unfinished_paths_stats.pkl\")\n",
    ")\n",
    "\n",
    "article_metrics_with_categories.to_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"article_metrics_with_categories.pkl\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code reads that version of the dataframe from the file:\n",
    "finished_paths = pd.read_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"finished_paths_stats.pkl\")\n",
    ")\n",
    "unfinished_paths = pd.read_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"unfinished_paths_stats.pkl\")\n",
    ")\n",
    "article_metrics_with_categories = pd.read_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"article_metrics_with_categories.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also load semantic distance df - TODO: delete this later when semantic distance is included in this notebook\n",
    "semantic = pd.read_pickle(\n",
    "    os.path.join(GENERATED_METRICS, \"semantic_df.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"everything\"></a>\n",
    "\n",
    "## 3 - Putting Everything Together: Logistic Regression\n",
    "We build a Logistic Regression  to determine influencing factors on the propensity of a player to give up a game (restart or timeout). We combine all factors we have explored before to build a regression model to predict whether a player gives up to then interpret the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataprep\"></a>\n",
    "\n",
    "#### 3.1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add semantic distance measures from start to target article\n",
    "semantic[\"sims_start_target\"] = semantic.sims_to_target.apply(lambda x: x[0])\n",
    "\n",
    "semantic_cols = [\n",
    "                # identifiers\n",
    "                'hashedIpAddress',\n",
    "                'timestamp',\n",
    "                'finished',\n",
    "\n",
    "                # metrics\n",
    "                'quit_freq',\n",
    "                'path_sims',\n",
    "                'sims_to_target',\n",
    "                'avg_sim_to_target',\n",
    "                'std_sim_to_target',\n",
    "                'sims_start_target']\n",
    "\n",
    "\n",
    "finished_paths = pd.merge(finished_paths, semantic.loc[semantic.finished==True, semantic_cols], how=\"left\", on=[\"hashedIpAddress\", \"timestamp\"]).drop(\"finished\", axis=1)\n",
    "unfinished_paths = pd.merge(unfinished_paths, semantic.loc[semantic.finished==False, semantic_cols], how=\"left\", on=[\"hashedIpAddress\", \"timestamp\"]).drop(\"finished\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We first merge the article metrics (categories, word count etc.) to the finished and unfinished paths to create a dataset\n",
    "\n",
    "# Merge and unfinished paths while adding a flag\n",
    "finished_paths[\"give_up\"] = 0\n",
    "unfinished_paths[\"give_up\"] = 1\n",
    "games = pd.concat((finished_paths, unfinished_paths), axis=0)\n",
    "\n",
    "# Drop duplicates in the article column, since one article might belong to more than one main category, some rows are duplicated. For now, we just drop these\n",
    "article_metrics_with_categories = article_metrics_with_categories.drop_duplicates(\n",
    "    subset=\"article\"\n",
    ")\n",
    "\n",
    "# Define columns that may be relevant from article metrics:\n",
    "# Some columns are exluded since they are contained in others, or because they are complements (stopword vs non-stopword percentage)\n",
    "keep = [\n",
    "    \"article\",\n",
    "    \"broad_category\",\n",
    "    \"paragraph_count\",\n",
    "    \"readability_score\",\n",
    "    \"stopword_percentage\",\n",
    "    \"avg_word_length\",\n",
    "    \"avg_sent_length\",\n",
    "]\n",
    "\n",
    "# Merge on start\n",
    "start_metrics = article_metrics_with_categories[keep].add_prefix(\"start_\")\n",
    "games = pd.merge(\n",
    "    games, start_metrics, how=\"left\", left_on=\"start\", right_on=\"start_article\"\n",
    ")  # add prefix\n",
    "print(games.shape)  # check results of the merge\n",
    "\n",
    "# Merge on target\n",
    "target_metrics = article_metrics_with_categories[keep].add_prefix(\"target_\")\n",
    "games = pd.merge(\n",
    "    games, target_metrics, how=\"left\", left_on=\"target\", right_on=\"target_article\"\n",
    ")  # add prefix\n",
    "print(games.shape)  # check results of the merge\n",
    "\n",
    "# Subset games to only include those with reasonable path lengths - TBD, no filtering for now\n",
    "# games = games[(games.path_length > 1) & (games.path_length < 30)]\n",
    "\n",
    "\n",
    "# remove unnecessary columns\n",
    "to_drop = [\n",
    "    \"hashedIpAddress\",\n",
    "    \"timestamp\",\n",
    "    \"durationInSec\",\n",
    "    \"path\",\n",
    "    \"rating\",\n",
    "    \"datetime\",\n",
    "    \"start\",\n",
    "    \"target\",\n",
    "    \"path_length\",\n",
    "    \"target_article\",\n",
    "    \"start_article\",\n",
    "    \"type\",\n",
    "    \n",
    "    # all semantic features not known before the game has ended\n",
    "    'quit_freq',\n",
    "    'path_sims',\n",
    "    'sims_to_target',\n",
    "    'avg_sim_to_target',\n",
    "    'std_sim_to_target',\n",
    "]\n",
    "games = games.drop(to_drop, axis=1)\n",
    "print(games.shape)  # check results of the subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = games.isnull().sum() * 100 / len(games)\n",
    "display(percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all NAs - for the actual analysis we need to investigate more why these are coming from.\n",
    "# For this proof of concept we just remove them\n",
    "games = games.dropna(axis=0, how=\"any\")\n",
    "display(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize all numerical columns\n",
    "cats = [\"start_broad_category\", \"target_broad_category\", \"give_up\"]\n",
    "continous = [col for col in games.columns if col not in cats]\n",
    "\n",
    "games.loc[:, continous] = (games.loc[:, continous] - games.loc[:, continous].mean())/games.loc[:, continous].std()\n",
    "games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg_all\"></a>\n",
    "\n",
    "#### 3.2 - Logistic Regression with all Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formula for logistic regression\n",
    "target = \"give_up\"\n",
    "predictors = [col for col in games.columns if col != target]\n",
    "formula = target + \" ~ \" + \" + \".join(predictors)\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model with the full formula (i.e. all relevant predictors)\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())\n",
    "fig, ax = create_coefplot(res)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg_subset\"></a>\n",
    "\n",
    "#### 3.3 - Logistic Regression with predictors known to player before the game\n",
    "we exclude anything not visible to the player at the start (e.g., the article metrics of the target article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model with limited predictors: only those visible to a player at the start of the game\n",
    "formula = \"give_up ~ links_to_target + shortest_path_length + start_broad_category + start_paragraph_count + start_readability_score + start_stopword_percentage + start_avg_sent_length + C(target_broad_category)\"\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())\n",
    "fig, ax = create_coefplot(res)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two regression models are largely congruent and offer some interesting initial findings (non-exhaustive):\n",
    "- some categories have a large and statistically significant influence on the proabability of a player giving up. A few examples:\n",
    "    - paths starting from *language and literature* or more niche topics like *Design and Technology* increase the propensity to give up\n",
    "    - a target article in the categories *Geography* or *Countries* strongly decreases the probability. This is consistent with the hypothesis that these are rather \"easy\" categories, as many links point to them.\n",
    "- similarly, many of the article related metrics are statistically significant; for instance:\n",
    "    - the shortest_path_length has a large positive coefficient, indicating that objectively longer paths do lead to more failures\n",
    "    - more detailed article metrics are statistically relevant, but the effect sizes are quite small (e.g., in-degree of target, readability score etc.)\n",
    "\n",
    "**Caveat:** Input features need to be standardized for easier interpretability and comparison. This will be done for P3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Machine Learning\n",
    "We will now try to model the probability of quitting in two ways:\n",
    "- same as for the logistic regression, i.e. in a tabular settings where we use all information known before the game\n",
    "- in a click by click fashion, where we try to model the quitting probability at a given page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 regular Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"give_up\"\n",
    "\n",
    "y = games[target]\n",
    "X = games.drop(target, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, stratify=y, shuffle=True)\n",
    "\n",
    "# One hot encoding for categories\n",
    "cats = [\"start_broad_category\", \"target_broad_category\"]\n",
    "X_train = pd.concat((X_train.drop(cats, axis=1), pd.get_dummies(X_train[cats])), axis=1)\n",
    "X_test = pd.concat((X_test.drop(cats, axis=1), pd.get_dummies(X_test[cats])), axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.sum() / len(y_train)) # we have a slight class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate for Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               max_depth=14,\n",
    "                               min_samples_split=2,\n",
    "                               class_weight={1: 2}) # 56% F1 score to beat, these are best params\n",
    "\n",
    "cross_validate(model, X_train, y_train, scoring=[\"f1\", \"accuracy\", \"precision\", \"recall\"], \n",
    "               return_train_score=True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate Gradient Boosting\n",
    "model = GradientBoostingClassifier()\n",
    "cross_validate(model, X_train, y_train, scoring=[\"f1\", \"accuracy\", \"precision\", \"recall\"],\n",
    "               return_train_score=True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a good model found through heuristic tuning with the CV settings above\n",
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               max_depth=14,\n",
    "                               min_samples_split=2,\n",
    "                               class_weight={1: 2.5})\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise confusion matrix on the test set and print performance metrics\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shapley explanation son the test set\n",
    "# global explainer\n",
    "\n",
    "test_subset = X_test.sample(1_000)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(test_subset)\n",
    "shap.summary_plot(shap_values, test_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanations\n",
    "choosen_instance = X_test.iloc[100]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Page by Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gather all data in a page per page manner\n",
    "from itertools import accumulate\n",
    "\n",
    "# add IDs first for all games\n",
    "finished_paths[\"game_id\"] = \"F\" + pd.Series(finished_paths.index).astype(str)\n",
    "unfinished_paths[\"game_id\"] = \"U\" + pd.Series(unfinished_paths.index).astype(str)\n",
    "\n",
    "# collecting the necessary information per page visited\n",
    "def get_all_pages(df):\n",
    "    out = []\n",
    "    for index, row in df.iterrows():\n",
    "        links = row['path']\n",
    "        dist_to_target = row['sims_to_target']\n",
    "\n",
    "        target_gain = [next - this for next, this in zip(dist_to_target, dist_to_target[1:] + [0])]\n",
    "        \n",
    "        max_gain = list(accumulate(dist_to_target, max))\n",
    "        loss_max_gain = [current - max for current, max in zip(dist_to_target, max_gain)]\n",
    "        cumulative_target_gain = np.cumsum(target_gain)\n",
    "\n",
    "\n",
    "        # get similarity to previous page. currently, index i stores btw i and i+1, so we insert \n",
    "        # similarity_next = row['path_sims']\n",
    "        # currently not working, incosistent number of similarity indexes\n",
    "        \n",
    "        game_id = row[\"game_id\"]\n",
    "\n",
    "        clicks = 1\n",
    "        for page, target_dist, gain, lost_gain, cum_gain in zip(links, dist_to_target, target_gain, loss_max_gain, cumulative_target_gain):\n",
    "            clicks += 1\n",
    "            out.append([game_id, page, clicks, target_dist, gain, lost_gain, cum_gain])\n",
    "\n",
    "    out = pd.DataFrame(out, columns=[\"game_id\", \"page\", \"num_step\", \"semantic_target_dist\", \"semantic_gain\", \"semantic_regress\", \"semantic_cum_gain\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_clicks = get_all_pages(finished_paths)\n",
    "finished_clicks[\"give_up\"] = 0\n",
    "\n",
    "unfinished_paths_sample = unfinished_paths[unfinished_paths.path_length > 2]\n",
    "unfinished_clicks = get_all_pages(unfinished_paths_sample)\n",
    "unfinished_clicks[\"give_up\"] = 0\n",
    "unfinished_clicks.loc[unfinished_clicks.groupby('game_id').tail(1).index, 'give_up'] = 1\n",
    "unfinished_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clicks = pd.concat((finished_clicks.sample(len(finished_clicks)), unfinished_clicks), axis=0)\n",
    "all_clicks = all_clicks.merge(article_metrics_with_categories[keep], how=\"left\", left_on=\"page\", right_on=\"article\")\n",
    "all_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = pd.concat((finished_paths, unfinished_paths), axis=0)\n",
    "\n",
    "features = [\n",
    "        'links_to_target',\n",
    "        'shortest_path_length',\n",
    "        'links_from_source',\n",
    "        # 'quit_freq', # test with and without, since this is kindof data leakage leaving it in\n",
    "        'sims_start_target',\n",
    "\n",
    "        # add identifier too\n",
    "        \"game_id\"\n",
    "]\n",
    "\n",
    "all_clicks = pd.merge(all_clicks, all_paths[features], how=\"left\", on=\"game_id\")\n",
    "all_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clicks.give_up.sum() / len(all_clicks.index) # heavy class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data split according to games\n",
    "\n",
    "test_size =  0.4\n",
    "IDS = [\"page\", \"article\", \"game_id\"]\n",
    "\n",
    "all_clicks = all_clicks.dropna(how=\"any\", axis=0)\n",
    "\n",
    "\n",
    "ids_finished = finished_paths.game_id\n",
    "ids_unfinished = unfinished_paths.game_id\n",
    "\n",
    "test_ids = np.random.choice(ids_finished, size=int(test_size * len(ids_finished))).tolist() + np.random.choice(ids_unfinished, size=int(test_size * len(ids_unfinished))).tolist()\n",
    "\n",
    "test_data = all_clicks[all_clicks.game_id.isin(test_ids)]\n",
    "train_data = all_clicks[~all_clicks.game_id.isin(test_ids)]\n",
    "\n",
    "test_data = test_data.drop(IDS, axis=1)\n",
    "train_data = train_data.drop(IDS, axis=1)\n",
    "\n",
    "y_train = train_data.give_up\n",
    "X_train = train_data.drop(\"give_up\", axis=1)\n",
    "\n",
    "y_test = test_data.give_up\n",
    "X_test = test_data.drop(\"give_up\", axis=1)\n",
    "\n",
    "print(\"Size of Training Set: {} \\n Size of Testing Set {}\".format(\n",
    "    X_train.shape,\n",
    "    X_test.shape\n",
    "))\n",
    "print(\"Share of Giving up Pages: \\n Train: {} \\n Test: {}\".format(\n",
    "    y_train.mean(),\n",
    "    y_test.mean()\n",
    "))\n",
    "\n",
    "print(\"Features used: {}\".format(X_train.columns.tolist()))\n",
    "\n",
    "\n",
    "# One hot encoding for categories\n",
    "cats = [\"broad_category\"]\n",
    "X_train = pd.concat((X_train.drop(cats, axis=1), pd.get_dummies(X_train[cats])), axis=1)\n",
    "X_test = pd.concat((X_test.drop(cats, axis=1), pd.get_dummies(X_test[cats])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = X_train.isnull().sum() * 100 / len(games)\n",
    "display(percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.dropna(how=\"any\", axis=0)\n",
    "X_test = X_test.dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate for Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               max_depth=14,\n",
    "                               min_samples_split=2,\n",
    "                               max_samples=0.6,\n",
    "                               max_features=1.0,\n",
    "                               class_weight={1: 10},\n",
    "                               n_jobs=-3) # 56% F1 score to beat, these are best params\n",
    "\n",
    "cross_validate(model, X_train, y_train, scoring=[\"f1\", \"accuracy\", \"precision\", \"recall\"], \n",
    "               return_train_score=True, cv=5, n_jobs=-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               max_depth=14,\n",
    "                               min_samples_split=2,\n",
    "                               class_weight={1: 10},\n",
    "                               n_jobs=-3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "evaluate_predictions(y_test, y_pred) # 97.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = X_test.sample(10_000)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(test_subset)\n",
    "shap.summary_plot(shap_values, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local explanations\n",
    "choosen_instance = X_test.iloc[-1]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Try to get the best predictive performance with all we have\n",
    "i.e. after the fact predictions wether a given path has finished, to understand the factors with shapley valuess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if helpful and necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Interactive plots for the data story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 - Availability of data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a histogram to show the number of games played over time.\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=finished_paths[\"datetime\"], name=\"Finished paths\", marker_color='lightgreen'))\n",
    "fig.add_trace(go.Histogram(x=unfinished_paths[\"datetime\"]), name=\"Unfinished paths\", marker_color='lightsalmon')\n",
    "\n",
    "fig.update_layout(\n",
    "     barmode='stack',\n",
    "     )\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Availability of data over time\",\n",
    "        'y':0.85,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Count\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - Categories of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe of the probabilities of not reaching certain categories of targets\n",
    "probs_df = pd.DataFrame()\n",
    "probs_df[\"category\"] = list(cat_unfinished_prob.keys())\n",
    "probs_df[\"prob\"] = cat_unfinished_prob.values()\n",
    "probs_df[\"err_below\"] = list(cat_err[0])\n",
    "probs_df[\"err_above\"] = list(cat_err[1])\n",
    "\n",
    "# Creating labels for the rating of probabilities, and assigning colours to them\n",
    "labels = ['Low', 'Average', 'High']\n",
    "colours = {'Low': 'lightgreen',\n",
    "          'Average': 'skyblue',\n",
    "          'High': 'lightsalmon'}\n",
    "bins = [0, 0.45, 0.55, 1]\n",
    "\n",
    "# Assigning labels to each of the categories\n",
    "probs_df[\"Probability rating\"] = pd.cut(list(cat_unfinished_prob.values()), bins=bins, labels=labels)\n",
    "\n",
    "# Sorting the dataframe from easy to hard categories\n",
    "probs_df = probs_df.sort_values(by=['prob'])\n",
    "\n",
    "# Creating interactive horizontal bar plots for the probability of not reaching certain categories of targets\n",
    "fig = px.bar(probs_df, x=\"prob\", y=\"category\", orientation='h', color=\"Probability rating\", color_discrete_map=colours, \n",
    "             error_x=\"err_above\", error_x_minus=\"err_below\")\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Empirical likelihood of not reaching certain categories of targets\",\n",
    "        'y':0.93,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Probability of not finishing a game\",\n",
    "    yaxis_title=\"Category of target\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create html code to embed in the data story\n",
    "fig.to_html(full_html = False, include_plotlyjs='cdn')\n",
    "fig.write_html(os.path.join(DATASTORY_PLOTS, \"likelihood_target_category.html\"), full_html=False, include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 - In-Degrees of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning colours to the finished and unfinished paths\n",
    "path_colours = {'Finished paths': 'lightgreen',\n",
    "          'Unfinished paths': 'lightsalmon'}\n",
    "\n",
    "# Drawing interactive box plots of the in-degrees of target articles\n",
    "fig = px.box(df_links, x=\"links_to_target\", color=\"path_type\", log_x=True, labels={\"path_type\": \"\"}, color_discrete_map=path_colours)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Distribution of the in-degrees of target articles\",\n",
    "        'y':0.93,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"In-degree of the target\",\n",
    "    yaxis_title=\"\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 - Shortest possible path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an overlaid interactive histogram of the shortest possible path lengths in games\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=shortest_possible_finished, histnorm='probability density', name=\"Finished paths\", marker_color='lightgreen'))\n",
    "fig.add_trace(go.Histogram(x=shortest_possible_unfinished, histnorm='probability density', name=\"Unfinished paths\", marker_color='lightsalmon'))\n",
    "fig.update_layout(\n",
    "    barmode='overlay',\n",
    "    )\n",
    "fig.update_traces(opacity=0.7)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Distribution of the shortest possible path lengths\",\n",
    "        'y':0.85,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title=\"Length of the shortest possible path length to complete the path\",\n",
    "    yaxis_title=\"Probability density\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 - Exploring Difference in Subject Strength Between Finished vs Uninished Path Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating interactive plot for section 2.1.2 - Exploring Subject Strength in Articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5.1 - Visualizing the strength between linked articles at a category level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\n",
    "    '#1f78b4',  # Blue\n",
    "    '#33a02c',  # Green\n",
    "    '#e41a1c',  # Red\n",
    "    '#984ea3',  # Purple\n",
    "    '#ff7f00',  # Orange\n",
    "    '#a65628',  # Brown\n",
    "    '#a68428',  # Golden\n",
    "    '#de53ed',  # Pink\n",
    "    '#ffff33',  # Yellow\n",
    "    '#28a65f',  # Strong Green\n",
    "    '#8dd3c7',  # Lighter Teal\n",
    "    '#b3b3b3',  # Gray\n",
    "    '#fdb462',  # Lighter Brown\n",
    "    '#b2df8a',  # Lighter Green\n",
    "    '#ffed6f',  # Lighter Yellow\n",
    "]\n",
    "color_map = {node: color_list[i] for i, node in enumerate(subject_graph.nodes())}\n",
    "\n",
    "for i, node in enumerate(subject_graph.nodes()):\n",
    "    subject_graph.nodes[node][\"size\"] = 20\n",
    "    subject_graph.nodes[node][\"color\"] = color_map[node]\n",
    "\n",
    "    outgoing_edges = [(u, v, subject_graph[u][v][\"size\"]) for u, v in subject_graph.out_edges(node)]\n",
    "    total_outgoing = sum(size for u, v, size in outgoing_edges)\n",
    "    # total_incomming = sum(size for u, v, size in [(u, v, subject_graph[u][v][\"size\"]) for u, v in subject_graph.in_edges(node)])\n",
    "    outgoing_edges = sorted(outgoing_edges, key=lambda edge: edge[2], reverse=True)\n",
    "\n",
    "    # hover_message = \"{} Incomming Edges from {}.\\n\".format(total_incomming, node)\n",
    "    hover_message = \"{} Outgoing Edges from {}.\\n\".format(total_outgoing, node)\n",
    "    for u, v, size in outgoing_edges:\n",
    "        hover_message += \"-> {}: {}\\n\".format(v.ljust(25), size)\n",
    "\n",
    "    subject_graph.nodes[node][\"hover\"] = hover_message\n",
    "\n",
    "for edge in subject_graph.edges():\n",
    "    subject_graph.edges[edge][\"color\"] = color_map[edge[0]]\n",
    "    # subject_graph.edges[edge][\"color\"] = \"grey\"\n",
    "\n",
    "# Add positions to graph\n",
    "pos = nx.shell_layout(subject_graph)\n",
    "for node in subject_graph.nodes():\n",
    "    subject_graph.nodes[node][\"x\"] = pos[node][0] * 200\n",
    "    subject_graph.nodes[node][\"y\"] = pos[node][1] * 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = gv.d3(\n",
    "  subject_graph,\n",
    "  graph_height=800,\n",
    "  show_details_toggle_button=False,\n",
    "  node_drag_fix=True,\n",
    "  node_hover_neighborhood=True,\n",
    "  node_label_size_factor=0.8,\n",
    "  node_size_factor=1.1,\n",
    "  use_edge_size_normalization=True,\n",
    "  # edge_curvature=0.1,\n",
    "  zoom_factor=1.6,\n",
    ")\n",
    "figure.export_html(os.path.join(DATASTORY_PLOTS, \"article_category_strength.html\"), overwrite=True)\n",
    "figure.display(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5.2 - Visualizing the difference in start-target category pairs between finish and unfinished paths.\n",
    "Node colors:\n",
    "* Based on the category difficulty (section 5.1)\n",
    "\n",
    "Edge colors:\n",
    "* Green: the start-target category link appeared more in the finished paths (“easy”, known to be solvable)\n",
    "* Red: the start-target category link appeared more in the unfinished paths (“harder”)\n",
    "* Gray: the start-target category link (an edge) is equally likely to be reached in finished and unfinished paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfinished_graph.edges() doesn't have (\"Mathematics\", \"Art\")\n",
    "# finished_graph.edges() doesn't have ('Art', 'Mathematics')\n",
    "\n",
    "diff_finished_unfinished_graph = nx.DiGraph()\n",
    "\n",
    "# Create a mapping between edge pairs and their corresponding indices\n",
    "edge_order = set(finished_graph.edges()).union(set(unfinished_graph.edges()))\n",
    "finished_edge_mapping = {(u, v): i for i, (u, v) in enumerate(finished_graph.edges())}\n",
    "unfinished_edge_mapping = {(u, v): i for i, (u, v) in enumerate(unfinished_graph.edges())}\n",
    "\n",
    "# Defnining colours for the nodes based on their probability rating (calculated in 5.2)\n",
    "color_map = probs_df.set_index('category')['Probability rating'].map(colours).to_dict()\n",
    "\n",
    "# Add nodes to graph\n",
    "for node in set(finished_graph.nodes()).union(set(unfinished_graph.nodes())):\n",
    "    diff_finished_unfinished_graph.add_node(node, size=20, color=color_map[node]) # Using the color_map defined \n",
    "\n",
    "# Add edges to graph\n",
    "edge_widths = []\n",
    "edge_colors = []\n",
    "for u, v in edge_order:\n",
    "  if (u, v) in finished_edge_mapping:\n",
    "    finished_edge_weight = finished_edge_widths[finished_edge_mapping[(u, v)]]\n",
    "  else:\n",
    "    finished_edge_weight = 0\n",
    "  \n",
    "  if (u, v) in unfinished_edge_mapping:\n",
    "    unfinished_edge_weight = unfinished_edge_widths[unfinished_edge_mapping[(u, v)]]\n",
    "  else:\n",
    "    unfinished_edge_weight = 0\n",
    "  \n",
    "  # Calculate the difference in edge weights between the finished and unfinished paths\n",
    "  width_diff = (finished_edge_weight + 1) / (unfinished_edge_weight + 1)\n",
    "  if width_diff <= 1:\n",
    "     # Appeared more in unfinished paths compared to finished paths (\"harder\")\n",
    "     color = \"red\"\n",
    "  else:\n",
    "     # Appeared more in finished paths compared to unfinished paths (\"easy\")\n",
    "     color = \"green\"\n",
    "     width_diff = 1 / width_diff\n",
    "\n",
    "  opacity=0.8\n",
    "  if width_diff > 0.92:\n",
    "     color = \"grey\"\n",
    "     opacity=0.1\n",
    "  \n",
    "  diff_finished_unfinished_graph.add_edge(u, v, size=abs(width_diff), color=color, hover=\"Edge: ({} - {})\".format(u, v), opacity=opacity)\n",
    "\n",
    "# Add positions to graph\n",
    "pos = nx.shell_layout(diff_finished_unfinished_graph)\n",
    "for node in diff_finished_unfinished_graph.nodes():\n",
    "    diff_finished_unfinished_graph.nodes[node][\"x\"] = pos[node][0] * 200\n",
    "    diff_finished_unfinished_graph.nodes[node][\"y\"] = pos[node][1] * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratio of finsihsed vs unfinished paths category strength\n",
    "# Values close to 1 indicate that the category is equally likely to be reached in finished and unfinished paths\n",
    "# Then we filter and focus on the observed discrepancies\n",
    "size_list = [diff_finished_unfinished_graph.get_edge_data(node, neighbour)[\"size\"] for node, neighbour in diff_finished_unfinished_graph.edges()]\n",
    "plt.hist(size_list, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = gv.d3(\n",
    "  diff_finished_unfinished_graph,\n",
    "  graph_height=800,\n",
    "  show_details_toggle_button=False,\n",
    "  node_drag_fix=True,\n",
    "  node_hover_neighborhood=True,\n",
    "  node_label_size_factor=0.8,\n",
    "  node_size_factor=1.1,\n",
    "  use_edge_size_normalization=True,\n",
    "  edge_curvature=0.1,\n",
    "  zoom_factor=1.6,\n",
    ")\n",
    "figure.export_html(os.path.join(DATASTORY_PLOTS, \"finish-unfinish_category_strength.html\"), overwrite=True)\n",
    "figure.display(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 - Exploring probabilities of reaching certain target countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we explore the likelihood of reaching certain target countries, as a complement to the analysis of the target categories. We estimate the likelihood of reaching a specific country in a similar fashion to that of the target categories, but we use an add-1 estimator instead of the empirical estimator due to the lower availability of data per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe of countries\n",
    "countries_df = categories[categories[\"broad_category\"] == \"Countries\"]\n",
    "\n",
    "# Get the paths with a country as a target\n",
    "country_finished_paths = finished_paths_post2011[finished_paths_post2011[\"target\"].isin(list(categories[categories[\"broad_category\"] == \"Countries\"][\"article\"]))].copy()\n",
    "country_unfinished_paths = unfinished_paths[unfinished_paths[\"target\"].isin(list(categories[categories[\"broad_category\"] == \"Countries\"][\"article\"]))].copy()\n",
    "\n",
    "# Count the occurances of countries as targets in finished and unfinished paths\n",
    "country_finished_count = country_finished_paths.groupby(\"target\").size()\n",
    "country_unfinished_count = country_unfinished_paths.groupby(\"target\").size()\n",
    "\n",
    "# Merge the country countrs into the dataframe of countries\n",
    "countries_df = countries_df.merge(country_finished_count.rename('finished_count'), how='left', left_on=\"article\", right_on=\"target\")\n",
    "countries_df = countries_df.merge(country_unfinished_count.rename('unfinished_count'), how='left', left_on=\"article\", right_on=\"target\")\n",
    "\n",
    "# Add 1 to each count for use in an add-1 estimator of the probabilities of finishing or not\n",
    "countries_df[\"finished_count\"] = countries_df[\"finished_count\"].apply(lambda x: x+1)\n",
    "countries_df[\"unfinished_count\"] = countries_df[\"unfinished_count\"].apply(lambda x: x+1)\n",
    "countries_df[\"finished_count\"] = countries_df[\"finished_count\"].fillna(1)\n",
    "countries_df[\"unfinished_count\"] = countries_df[\"unfinished_count\"].fillna(1)\n",
    "\n",
    "# Add-1 estimator for probablilities of reaching/not reaching target countries\n",
    "for index, row in countries_df.iterrows():\n",
    "    countries_df.at[index, \"Probability of completion\"] = row[\"finished_count\"] / (row[\"unfinished_count\"] + row[\"finished_count\"])\n",
    "    countries_df.at[index, \"Probability of failure\"] = row[\"unfinished_count\"] / (row[\"unfinished_count\"] + row[\"finished_count\"])\n",
    "\n",
    "# Map the countries to iso-alpha-3 codes\n",
    "country_codes = country_codes_dict()\n",
    "countries_df[\"iso_alpha\"] = countries_df[\"article\"].apply(lambda x: country_codes[x] if x in country_codes.keys() else None)\n",
    "\n",
    "countries_df[\"Probability of completion\"] = countries_df[\"Probability of completion\"].astype(float)\n",
    "countries_df[\"Probability of failure\"] = countries_df[\"Probability of failure\"].astype(float)\n",
    "\n",
    "# Plot map of countries, coloured by their likelihood of being reached as targets\n",
    "fig = px.choropleth(countries_df, locations=\"iso_alpha\",\n",
    "                    color=\"Probability of completion\",\n",
    "                    hover_name=\"article\",\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Probability of reaching a target country\",\n",
    "        'y':0.93,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
