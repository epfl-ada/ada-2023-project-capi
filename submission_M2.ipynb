{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA CAPI Notebook for Project Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "1. [Loading and Preparing the Data](#load)\n",
    "    1. [Load Tabular Data](#tabu)\n",
    "    2. [Clean Tabular Data](#clean)\n",
    "2. [Data Extraction](#extract)\n",
    "    1. [Extracting metrics from textual articles](#gen)\n",
    "        1. [Defining Article Metrics](#define_arti_metrics)\n",
    "        2. [Extracting Article Metrics](#extracting_arti_metrics)\n",
    "3. [Data Analysis](#analysis)\n",
    "    1. [Exploring Path Lengths](#paths)\n",
    "    2. [Exploring Categories in the Paths](#cats)\n",
    "    3. [Exploring Subject Strength in Articles](#sub)\n",
    "        1. [Exploring Subject Strength in Connected Articles](#graph_cat)\n",
    "        2. [Exploring Subject Strength in Finished Path Articles](#graph_cat_fi)\n",
    "        3. [Exploring Subject Strength in Uninished Path Articles](#graph_cat_unfi)\n",
    "    4. [Analysing Article Metrics](#artmet)\n",
    "        1. [Analysing Article Metrics by Category](#artmet_cat)\n",
    "        2. [Analysing Article Metrics in Finished vs Unfinished paths](#artmetfu_path)\n",
    "    5. [Analysing the In-Degree of Targets in Finished vs Unfinished Paths](#ltt)\n",
    "    6. [Analysing Possible Shortest Path Distances in Finished vs Unfinished Paths](#shortest)\n",
    "4. [Putting Everything Together](#everything)\n",
    "    1. [Exploration per Actual Link](#actlink)\n",
    "5. [Initial Regression](#regression)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats \n",
    "\n",
    "# Helper functions from utils folder\n",
    "from utils.analysis import t_test_article_metrics, visualize_article_connections_per_category\n",
    "from utils.preprocessing import get_all_links, merge_articles_categories, create_category_dictionaries\n",
    "\n",
    "# Formatting libraries\n",
    "import urllib\n",
    "import datetime as datetime\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Imports to perform article analysis\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt') # Punkt tokenizer\n",
    "nltk.download('stopwords') # Commong stopwords\n",
    "\n",
    "# Load config and extract variables\n",
    "import config\n",
    "DATA_PATH = config.PATH_TO_DATA\n",
    "PATH_GRAPGH_FOLDER = \"wikispeedia_paths-and-graph\"\n",
    "ARTICLE_FOLDER = \"plaintext_articles\"\n",
    "GENERATED_METRICS = \"generated_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 1 - Loading and Preparing the Data\n",
    "\n",
    "Note that you can load the data from [here](#checkpoint1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabu\"></a>\n",
    "#### 1.1 - Load Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all data (except wikipedia articles)\n",
    "finished_paths = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"paths_finished.tsv\"), sep='\\t', skiprows=15, \n",
    "                             names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"rating\"])\n",
    "unfinished_paths = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"paths_unfinished.tsv\"), sep='\\t', skiprows=16, \n",
    "                               names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"target\", \"type\"])\n",
    "edges = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"links.tsv\"), sep='\\t', skiprows=15, names=[\"start\", \"end\"], encoding=\"utf-8\")\n",
    "articles = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"articles.tsv\"), sep='\\t', skiprows=12, names=[\"article\"], encoding=\"utf-8\")\n",
    "categories = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"categories.tsv\"), sep='\\t', skiprows=13, \n",
    "                         names=[\"article\", \"category\"], encoding=\"utf-8\")\n",
    "shortest_paths = np.genfromtxt(os.path.join(DATA_PATH, PATH_GRAPGH_FOLDER, \"shortest-path-distance-matrix.txt\"), delimiter=1, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "#### 1.2 - Clean Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding in edge list\n",
    "display(edges.head())\n",
    "edges[\"start\"] = edges.start.apply(urllib.parse.unquote)\n",
    "edges[\"end\"] = edges.end.apply(urllib.parse.unquote)\n",
    "display(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datetime as datetime object\n",
    "finished_paths[\"datetime\"] = finished_paths.timestamp.apply(datetime.datetime.fromtimestamp)\n",
    "unfinished_paths[\"datetime\"] = unfinished_paths.timestamp.apply(datetime.datetime.fromtimestamp)\n",
    "display(unfinished_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for articles\n",
    "display(articles.head())\n",
    "articles[\"article\"] = articles.article.apply(urllib.parse.unquote)\n",
    "display(articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for categories\n",
    "display(categories.head())\n",
    "categories[\"article\"] = categories.article.apply(urllib.parse.unquote)\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify broad categories of articles\n",
    "display(categories.head())\n",
    "categories[\"broad_category\"] = categories[\"category\"].apply(lambda x: x.split(\".\")[1]) # first entry after subject.\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles and categories\n",
    "articles_categories = pd.merge(articles, categories, how=\"left\", on=\"article\")\n",
    "display(articles_categories.head())\n",
    "\n",
    "# 6 articles without category! # TODO: discuss: what do we do with these?\n",
    "print(\"Merge introduced {} NAs in category columns:\".format(articles_categories.category.isna().sum()))\n",
    "articles_categories[articles_categories.category.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paths to a readable format (lists) and remove url encoding\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(lambda x: [urllib.parse.unquote(y) for y in x])\n",
    "\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(lambda x: [urllib.parse.unquote(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and target articles of path\n",
    "finished_paths[\"start\"] = [path[0] for path in finished_paths[\"path\"]]\n",
    "finished_paths[\"target\"] = [path[-1] for path in finished_paths[\"path\"]]\n",
    "\n",
    "unfinished_paths[\"start\"] = [path[0] for path in unfinished_paths[\"path\"]]\n",
    "unfinished_paths[\"target\"] = unfinished_paths[\"target\"].apply(urllib.parse.unquote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extract\"></a>\n",
    "\n",
    "## 2 - Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gen\"></a>\n",
    "\n",
    "#### 2.1 - Extracting metrics from textual articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_arti_metrics\"></a>\n",
    "##### 2.1.1 - Defining Article Metrics\n",
    "\n",
    "The following metrics are extracted by performing textual pre-processing techniques in the wikipedia articles:\n",
    "* Total word count: To understand the length of the article.\n",
    "* Non stopword frequency: To identify words that contribute to the content's meaning.\n",
    "* Stopword frequency: To identify common words that may not contribute to the content's meaning.\n",
    "* Average word length: To assess the complexity of the language used.\n",
    "* Average sentence length: Longer or more complex sentences (based on characters) may contribute to frustration.\n",
    "* Number of paragraphs: To see if the article's structure plays a role in people giving up.\n",
    "* Keyword frequency: To identify the most common keywords to understand the article's focus.\n",
    "* Readability: To see if the ease of reading the article has an impact (metric: Flesch Reading Ease Score) Link: https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_article(article_text):\n",
    "    preprocessed_text = article_text\n",
    "    preprocessed_text = preprocessed_text.lower()\n",
    "    preprocessed_text = preprocessed_text.replace(\"\\n   \", \" \") # As the articles are not continuous sentences\n",
    "    return preprocessed_text\n",
    "\n",
    "def calculate_article_metrics(article_text):\n",
    "    preprocessed_text = proprocess_article(article_text)\n",
    "\n",
    "    words = word_tokenize(preprocessed_text)\n",
    "    sentences = sent_tokenize(preprocessed_text)\n",
    "\n",
    "    # Calculate total word count\n",
    "    total_word_count = len(words)\n",
    "\n",
    "    # Calculate stopword frequency\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stopwords_count = 0\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word.isalpha() and word.lower() in stop_words:\n",
    "            stopwords_count +=1\n",
    "        if word.isalpha() and word.lower() not in stop_words:\n",
    "            unique_words.append(word.lower())\n",
    "\n",
    "    # Calculate average word length\n",
    "    average_word_length = sum(len(word) for word in words) / total_word_count\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    average_sentence_length = sum(len(sentence) for sentence in sentences) / len(sentences)\n",
    "\n",
    "    # Calculate number of paragraphs (assume every new line \\n is paragraph)\n",
    "    paragraphs_count = preprocessed_text.count('\\n') + 1 # Count last paragraph\n",
    "\n",
    "    # Calculate keyword frequency\n",
    "    word_freq = nltk.FreqDist(unique_words)\n",
    "    most_common_words = word_freq.most_common(10)  # Parameter to adjust\n",
    "\n",
    "    # Calculate readability (Flesch Reading Ease Score) - 100: Easy to read, 0: Very confusing\n",
    "    readability = textstat.flesch_reading_ease(preprocessed_text)\n",
    "\n",
    "    return {\n",
    "        \"word_count\": total_word_count,\n",
    "        \"non_stopword_count\": total_word_count - stopwords_count,\n",
    "        \"non_stopword_percentage\": (total_word_count - stopwords_count) / total_word_count,\n",
    "        \"stopword_count\": stopwords_count,\n",
    "        \"stopword_percentage\": stopwords_count / total_word_count,\n",
    "        \"avg_word_length\": average_word_length,\n",
    "        \"avg_sent_length\": average_sentence_length,\n",
    "        \"paragraph_count\": paragraphs_count,\n",
    "        \"common_words\": most_common_words,\n",
    "        \"readability_score\": readability,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extracting_arti_metrics\"></a>\n",
    "##### 2.1.2 - Extracting Article Metrics\n",
    "\n",
    "The commented code below was used to access the `plaintext_articles` folder and read all articles inside, creating a dataframe with all the metric information (see table below). To reduce runtime, we compute the article metrics once and then read the generated CSV file.\n",
    "\n",
    "\n",
    "Article Metrics DataFrame Description:\n",
    "| Column Name                   | Metric                   | Purpose                                                            | Description                                                                                                  |\n",
    "|--------------------------|--------------------------|--------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| `word_count`         | Total Word Count         | To understand the overall length of the article.                   | Represents the total number of words in the article.                                                         |\n",
    "| `non_stopword_count`         | Non-Stopword Frequency   | To identify words that contribute to the content's meaning.         | Measures the frequency of non-stopwords, highlighting contextually relevant terms.                           |\n",
    "| `stopword_count`         | Stopword Frequency       | To identify common words that may not contribute significantly.   | Measures the frequency of stopwords, aiding in identifying less informative words.                           |\n",
    "| `avg_word_length`         | Average Word Length      | To assess the complexity of the language used.                     | Calculates the average length of words in the article.                                                        |\n",
    "| `avg_sent_length`         | Average Sentence Length  | To evaluate sentence complexity based on characters.              | Computes the average number of characters per sentence, providing insights into structure and readability.   |\n",
    "| `paragraph_count`        | Number of Paragraphs     | To assess the role of article structure in user engagement.        | Indicates the total number of paragraphs in the article.                                                      |\n",
    "| `common_words`         | Keyword Frequency        | To identify common keywords and understand the article's focus.    | Reveals the frequency of keywords, aiding in discerning prevalent themes within the content.                   |\n",
    "| `readability_score`         | Readability (Flesch Score)| To see if the ease of reading the article has an impact.                        | Utilizes the Flesch Reading Ease Score for readability assessment. [Learn more](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(DATA_PATH, ARTICLE_FOLDER)\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "\n",
    "  article_metrics = pd.DataFrame(columns=[\"article\", \"word_count\", \"non_stopword_count\", \"non_stopword_percentage\", \"stopword_count\", \"stopword_percentage\", \"avg_word_length\", \"avg_sent_length\", \"paragraph_count\", \"common_words\", \"readability_score\"])\n",
    "\n",
    "  for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "      root, extension = os.path.splitext(file_name)\n",
    "      readable_file_name = urllib.parse.unquote(root)\n",
    "      \n",
    "      with open(file_path, \"r\", encoding=\"utf-8\") as article:\n",
    "        metrics = calculate_article_metrics(article.read())\n",
    "\n",
    "        metrics[\"article\"] = readable_file_name\n",
    "        article_metrics.loc[len(article_metrics)] = metrics\n",
    "else:\n",
    "  raise FileNotFoundError(\"The specified folder path does not exist or is not a directory.\")\n",
    "\n",
    "article_metrics.to_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_metrics = pd.read_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(article_metrics.info())\n",
    "display(article_metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "\n",
    "## 3 -Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths\"></a>\n",
    "\n",
    "#### 3.1 - Exploring Path Lengths\n",
    "\n",
    "Compare the path lengths between the finished and unfinished paths to detect potential outliers or trends that might influence the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate path lengths for finished paths and show summary statistics\n",
    "finished_paths[\"path_length\"] = finished_paths.path.apply(lambda el: len(el))\n",
    "finished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate path lengths for unfinished paths and show summary statistics\n",
    "unfinished_paths[\"path_length\"] = unfinished_paths.path.apply(lambda el: len(el))\n",
    "unfinished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distributions of finished and unfinished paths\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "threshold = 40 # for now we remove some outliers to make the plots meaningful\n",
    "\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "sns.histplot(x=finished_paths.path_length[finished_paths.path_length < threshold], ax=ax, discrete=True, alpha=0.4)\n",
    "sns.histplot(x=unfinished_paths.path_length[unfinished_paths.path_length < threshold], ax=ax, discrete=True, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of path lengths across finished, restarted paths and unfinished paths that timed out\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "sns.histplot(x=finished_paths.path_length[finished_paths.path_length < threshold], ax=axes[0], discrete=True)\n",
    "axes[0].set_title(\"Finished Paths\")\n",
    "\n",
    "unfinished_clean = unfinished_paths[(unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"restart\")]\n",
    "sns.histplot(data=unfinished_clean, x=\"path_length\", ax=axes[1], discrete=True,)\n",
    "axes[1].set_title(\"Unfinished Paths - Restart\")\n",
    "\n",
    "unfinished_clean = unfinished_paths[(unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"timeout\")]\n",
    "sns.histplot(data=unfinished_clean, x=\"path_length\", ax=axes[2], discrete=True,)\n",
    "axes[2].set_title(\"Unfinished Paths - Timeout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are quite a few outliers and extreme values. For the analysis we have to think about removing them:\n",
    "- Do we remove games with just one click?\n",
    "- Do we remove games with a very high path length (e.g., above 30), since the player might just have clicked randomly?\n",
    "\n",
    "We plan to address these questions in the analysis, e.g., through a sensitivity analysis: We run our analysis first on the regular data, before checking if we get similar results while removing certain outlier games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cats\"></a>\n",
    "\n",
    "#### 3.2 - Exploring Categories in the Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the occurances of categories in the paths, to gain an understanding of whether certain categories lead to games that are on average easier for people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing which categories are most represented in articles\n",
    "count_articles = categories.groupby(\"broad_category\").size()\n",
    "\n",
    "print(\"Below shows how many articles each of the broad categories are represented by\")\n",
    "display(count_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for easy discovery of what categories an article belongs to\n",
    "article_to_category, article_to_broad_category = create_category_dictionaries(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each category has occured as a target in the finished and unfinished paths.\n",
    "# Note that some articles are represented by multiple categories, which are thus counted extra.\n",
    "\n",
    "# TODO: discuss: maybe wrap this in a function?\n",
    "\n",
    "all_target_broad_categories_f = [\n",
    "  article_to_broad_category[target] for target in finished_paths[\"target\"] if target in article_to_broad_category\n",
    "]\n",
    "all_target_broad_categories_f = [item for sublist in all_target_broad_categories_f for item in sublist]\n",
    "count_cats_finished_target = Counter(all_target_broad_categories_f)\n",
    "keys_finished = list(count_cats_finished_target.keys())\n",
    "keys_finished.sort()\n",
    "sorted_cats_f = {i: count_cats_finished_target[i] for i in keys_finished}\n",
    "\n",
    "all_target_broad_categories_u = [\n",
    "  article_to_broad_category[target] for target in unfinished_paths[\"target\"] if target in article_to_broad_category\n",
    "]\n",
    "all_target_broad_categories_u = [item for sublist in all_target_broad_categories_u for item in sublist]\n",
    "count_cats_unfinished_target = Counter(all_target_broad_categories_u)\n",
    "keys_unfinished = list(count_cats_unfinished_target.keys())\n",
    "keys_unfinished.sort()\n",
    "sorted_cats_u = {i: count_cats_unfinished_target[i] for i in keys_unfinished}\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(list(sorted_cats_f.keys()), sorted_cats_f.values(), label=\"Finished paths\")\n",
    "ax2 = plt.barh(list(sorted_cats_u.keys()), sorted_cats_u.values(), label=\"Unfinished paths\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.title(\"Occurences of categories as targets\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows clearly that some categories occur as targets relatively more in finished paths while others have the opposite outcome. For example, \"Couuntries\" occurs as a target in finished paths multiple times as often as it does in unfinished paths, whereas \"Everyday_life\" occurs as a target in finished paths only slightly more often than it does in unfinished paths. This indicates the possibility that certain categories make for easier games. Nonetheless, to establish a proper relationship, we may in the future need to control for certain other variables. For example, it could be that articles in the \"Countries\" category are simply better connected than those in the \"Everyday_life\" category. \n",
    "\n",
    "The plot also shows the imbalance in the categories. We have a lot of paths ending in \"Geography\" and \"Science\", but very few ending in \"Mathematics\" and \"Art\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sub\"></a>\n",
    "\n",
    "#### 3.3 - Exploring Subject Strength in Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat\"></a>\n",
    "\n",
    "##### 3.3.1 - Exploring Subject Strength in Connected Articles\n",
    "Visualizing the strength of the categories for connected articles (those which are connected by an edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing FINISHED PATHS article connections per category\n",
    "edge_category = merge_articles_categories(edges, [\"start\", \"end\"], articles_categories)\n",
    "visualize_article_connections_per_category(edge_category, \"Article Connections Based on Category (Normalized and Scaled Edges)\")\n",
    "\n",
    "# TODO: Should add some comments about these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_fi\"></a>\n",
    "\n",
    "##### 3.3.2 - Exploring Subject Strength in Finished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the finished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing FINISHED PATHS article connections per category\n",
    "finished_paths_categories = merge_articles_categories(finished_paths, [\"start\", \"target\"], articles_categories)\n",
    "visualize_article_connections_per_category(finished_paths_categories, \"Start & Target Article Connections in Finished Path Based on Category (Normalized and Scaled Edges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_unfi\"></a>\n",
    "\n",
    "##### 3.3.3 - Exploring Subject Strength in Uninished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the unfinished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing UNFINISHED PATHS article connections per category\n",
    "unfinished_paths_categories = merge_articles_categories(unfinished_paths, [\"start\", \"target\"], articles_categories)\n",
    "visualize_article_connections_per_category(unfinished_paths_categories, \"Start & Target Article Connections in Unfinished Path Based on Category (Normalized and Scaled Edges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet\"></a>\n",
    "\n",
    "#### 3.4 - Analysing Article Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet_cat\"></a>\n",
    "\n",
    "##### 3.4.1 - Analysing Article Metrics by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge articles with their corresponding categories\n",
    "article_metrics_with_categories = article_metrics.merge(categories, how=\"left\", on=[\"article\"])\n",
    "display(article_metrics_with_categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = ['word_count', 'stopword_count', 'stopword_percentage', 'non_stopword_count', 'non_stopword_percentage','avg_word_length', 'avg_sent_length', 'paragraph_count','readability_score']\n",
    "fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "  # Bar plot\n",
    "  ax_bar = axes[idx, 0]\n",
    "  sns.barplot(x=article_metrics_with_categories[\"broad_category\"], y=article_metrics_with_categories[metric], errorbar=(\"ci\", 95), ax=ax_bar)\n",
    "  ax_bar.set_xlabel(\"Category\")\n",
    "  ax_bar.set_ylabel(metric)\n",
    "  ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "  ax_bar.set_xticklabels(ax_bar.get_xticklabels(), rotation=90)\n",
    "\n",
    "  # Violin plot\n",
    "  ax_violin = axes[idx, 1]\n",
    "  sns.violinplot(x=article_metrics_with_categories[\"broad_category\"], y=article_metrics_with_categories[metric], ax=ax_violin)\n",
    "  ax_violin.set_xlabel(\"Category\")\n",
    "  ax_violin.set_ylabel(metric)\n",
    "  ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "  ax_violin.set_xticklabels(ax_violin.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmetfu_path\"></a>\n",
    "\n",
    "##### 3.4.2 - Analysing Article Metrics in Finished vs Unfinished paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the article metrics per finished and unfinished parths (both for start and end articles)\n",
    "start_finished_article_metrics = finished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\")\n",
    "end_finished_article_metrics = finished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\")\n",
    "start_unfinished_article_metrics = unfinished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\")\n",
    "end_unfinished_article_metrics = unfinished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\"word_count\", \"stopword_count\", \"stopword_percentage\", \"non_stopword_count\", \"non_stopword_percentage\",\"avg_word_length\", \"avg_sent_length\", \"paragraph_count\", \"readability_score\"]\n",
    "dataframes = [start_finished_article_metrics, start_unfinished_article_metrics, end_finished_article_metrics, end_unfinished_article_metrics]\n",
    "dataframe_labels = [\"Start Finished\", \"Start Unfinished\", \"Target Finished\", \"Target Unfinished\"]\n",
    "\n",
    "# TODO: is this right? Should the plots not show the metrics for start finished, start unfinished, target finished, target unfinished?\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "  data = [df[metric] for df in dataframes]\n",
    "  \n",
    "  # Bar plot\n",
    "  ax_bar = axes[idx, 0]\n",
    "  sns.barplot(data=data, errorbar=(\"ci\", 95), ax=ax_bar)\n",
    "  ax_bar.set_xlabel(\"Type of article\")\n",
    "  ax_bar.set_ylabel(metric)\n",
    "  ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "  ax_bar.set_xticklabels(dataframe_labels)\n",
    "\n",
    "  # Violin plot\n",
    "  ax_violin = axes[idx, 1]\n",
    "  sns.violinplot(data=data, ax=ax_violin)\n",
    "  ax_bar.set_xlabel(\"Type of article\")\n",
    "  ax_violin.set_ylabel(metric)\n",
    "  ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "  ax_violin.set_xticklabels(dataframe_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_finished_article_metrics, start_unfinished_article_metrics)\n",
    "\n",
    "print(\"\\nTarget Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(metrics_to_plot, end_finished_article_metrics, end_unfinished_article_metrics)\n",
    "\n",
    "\n",
    "# TODO: discuss: why are we comparing start and end article of the same game? what are we trying to do - add a commnet?!\n",
    "print(\"\\nFinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_finished_article_metrics, end_finished_article_metrics)\n",
    "\n",
    "print(\"\\nUnfinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_unfinished_article_metrics, end_unfinished_article_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric                     | Start Articles (Finished vs Unfinished) | Target Articles (Finished vs Unfinished) | Finished Articles (Start vs Target) | Unfinished Articles (Start vs Target) |\n",
    "|----------------------------|-----------------------------------------|------------------------------------------|--------------------------------------|----------------------------------------|\n",
    "| word_count                  | t-statistic: 1.873, p-value: 0.061       | t-statistic: 36.838, p-value: 0.000      | t-statistic: -30.824, p-value: 0.000 | t-statistic: 8.949, p-value: 0.000     |\n",
    "| stopword_count              | t-statistic: 1.353, p-value: 0.176       | t-statistic: 34.692, p-value: 0.000      | t-statistic: -30.009, p-value: 0.000 | t-statistic: 8.076, p-value: 0.000     |\n",
    "| stopword_percentage         | t-statistic: -3.366, p-value: 0.001      | t-statistic: 4.045, p-value: 0.000       | t-statistic: -0.362, p-value: 0.717 | t-statistic: 6.045, p-value: 0.000     |\n",
    "| non_stopword_count          | t-statistic: 2.131, p-value: 0.033       | t-statistic: 37.599, p-value: 0.000      | t-statistic: -30.943, p-value: 0.000 | t-statistic: 9.317, p-value: 0.000     |\n",
    "| non_stopword_percentage     | t-statistic: 3.366, p-value: 0.001       | t-statistic: -4.045, p-value: 0.000      | t-statistic: 0.362, p-value: 0.717  | t-statistic: -6.045, p-value: 0.000    |\n",
    "| avg_word_length             | t-statistic: -3.090, p-value: 0.002      | t-statistic: 10.974, p-value: 0.000     | t-statistic: 2.987, p-value: 0.003  | t-statistic: 14.113, p-value: 0.000   |\n",
    "| avg_sent_length             | t-statistic: 4.863, p-value: 0.000       | t-statistic: -0.260, p-value: 0.795     | t-statistic: -3.443, p-value: 0.001 | t-statistic: -6.964, p-value: 0.000    |\n",
    "| paragraph_count             | t-statistic: 0.038, p-value: 0.970       | t-statistic: 37.247, p-value: 0.000     | t-statistic: -29.294, p-value: 0.000| t-statistic: 11.952, p-value: 0.000   |\n",
    "| readability_score           | t-statistic: 4.652, p-value: 0.000       | t-statistic: -21.100, p-value: 0.000   | t-statistic: -8.953, p-value: 0.000 | t-statistic: -27.779, p-value: 0.000 |\n",
    "\n",
    "1. **Finished vs Unfinished Start Articles:**\n",
    "   - The stopword_percentage is significantly lower (and non_stopword_percentage higher) in finished articles than unfinished, suggesting a potential emphasis on more meaningful content. \n",
    "   - Finished start articles also tend to have higher avg_sent_length and readability_score, indicating a focus on well-structured and reader-friendly content.\n",
    "\n",
    "2. **Finished vs Unfinished Target Articles:**\n",
    "   - Finished target articles exhibit significantly higher values across various metrics, including word_count, stopword_percentage (with lower non_stopword_percentage), avg_word_length, and paragraph_count. Additionally, they have a significantly lower readability_score, suggesting that finished target articles could be more challenging to comprehend.\n",
    "\n",
    "3. **Finished Start vs. Target Articles:**\n",
    "   - TODO\n",
    "\n",
    "4. **Unfinished Target Articles:**\n",
    "   - TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ltt\"></a>\n",
    "\n",
    "#### 3.5 - Analysing the In-Degree of Targets in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that certain paths are easier objectively because their targets have a larger \"in-degree\", i.e. the number of edges in the graph pointing to it. This would be intuitive: if there are more ways to get to the target, it should be easier to do so. This section explores whether this idea is reflected in the distributions of the in-degrees of the targets in finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many links point to targets in finished and unfinished paths, known as the \"in-degree\".\n",
    "finished_paths[\"links_to_target\"] = finished_paths[\"path\"].apply(lambda x: len(edges.loc[edges[\"end\"] == x[-1]]))\n",
    "unfinished_paths[\"links_to_target\"] = unfinished_paths[\"target\"].apply(lambda x: len(edges.loc[edges[\"end\"] == x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that the in-degree may follow a power-law. We check this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the arrays of for the cumulative distributions of in-degrees:\n",
    "finished_indegree_cumulative=plt.hist(finished_paths.links_to_target,bins=100,log=True,cumulative=-1,histtype='step')\n",
    "unfinished_indegree_cumulative=plt.hist(unfinished_paths.links_to_target,bins=100,log=True,cumulative=-1,histtype='step')\n",
    "plt.close()\n",
    "\n",
    "# Plotting the CCDF plots of the in-degrees for finished and unfinished paths:\n",
    "plt.loglog(finished_indegree_cumulative[1][1:],finished_indegree_cumulative[0], label=\"Finished paths\")\n",
    "plt.loglog(unfinished_indegree_cumulative[1][1:],unfinished_indegree_cumulative[0], label=\"Unfinished paths\")\n",
    "plt.title('Histogram of In-degree (cumulative)')\n",
    "plt.ylabel('# of targets (in log scale)')\n",
    "plt.xlabel('In-degree (in log scale)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean in-degree of the targets in the finished and unfinished paths.\n",
    "print(\"The targets that were reached had an in-degree of {:.3f} on average.\".format(finished_paths['links_to_target'].mean()))\n",
    "print(\"The targets that were not reached had an in-degree of {:.3f} on average.\".format(unfinished_paths['links_to_target'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing median in-degree of the targets in the finished and unfinished paths.\n",
    "print(\"The targets that were reached had a median in-degree of {:.3f}.\".format(finished_paths['links_to_target'].median()))\n",
    "print(\"The targets that were not reached had a median in-degree of {:.3f}.\".format(unfinished_paths['links_to_target'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting a t-test\n",
    "t_test_article_metrics([\"links_to_target\"], finished_paths, unfinished_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the number of links pointing to the targets of finished and unfinished paths is 0.0. This means we reject the null hypothesis that the number of links pointing to the targets are statistically the same at the 5% level of significance, indicating that the in-degree of the target indeed has a statistical significance in whether a game will be finished or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boxplot of the trends\n",
    "\n",
    "finished_links =  pd.DataFrame()\n",
    "finished_links[\"links_to_target\"] = finished_paths[\"links_to_target\"]\n",
    "finished_links[\"path_type\"] = \"Finished paths\"\n",
    "\n",
    "unfinished_links =  pd.DataFrame()\n",
    "unfinished_links[\"links_to_target\"] = unfinished_paths[\"links_to_target\"]\n",
    "unfinished_links[\"path_type\"] = \"Unfinished paths\"\n",
    "\n",
    "df_links = pd.concat([finished_links,unfinished_links])\n",
    "\n",
    "ax = sns.boxplot(x=\"path_type\", y=\"links_to_target\", data=df_links)\n",
    "plt.xlabel(\" \")\n",
    "plt.ylim([-5,155])\n",
    "plt.ylabel(\"Number of links to target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above highlight these conclusions. The in-degree of targets in the finished paths are noticeably higher than those in the unfinished paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shortest\"></a>\n",
    "\n",
    "#### 3.6 - Analysing Possible Shortest Path Distances in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential factor that may determine whether a game will be completed or not, in a more objective manner, is the shortest path length possible between the source and the target. This factor is also intuitive. If a shorter path exists in theory, the path length should also be shorter on average in practice, leading to simpler games. This section explores whether this idea is reflected in the distributions of the length of the shortest possible paths in finished and unfinished games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the finished games\n",
    "\n",
    "finished_paths[\"shortest_path_length\"] = finished_paths[\"path\"].apply(\n",
    "    lambda x: shortest_paths[articles.loc[articles['article'] == x[0]].index[0]][articles.loc[articles['article'] == x[-1]].index[0]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: There are typos in the targets.\n",
    "\n",
    "Eg. At index 141 in unfinished paths, the target is written as \"Long_peper\", when it should be \"Long_pepper\".\n",
    "\n",
    "Overall, an issue arises in unfinished paths 28 times, but this doesn't seem to be an issue in finished paths. These data points are ignored so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the unfinished games\n",
    "\n",
    "# TODO: wrap in a function since we are using it twice (see below)?\n",
    "\n",
    "shortest_unfinished = []\n",
    "not_found = 0\n",
    "for i in range(len(unfinished_paths)):\n",
    "    source = articles.loc[articles['article'] == unfinished_paths.iloc[i][\"path\"][0]]\n",
    "    target = articles.loc[articles['article'] == unfinished_paths.iloc[i][\"target\"]]\n",
    "    if len(source) != 0 and len(target) != 0:\n",
    "        index_source = source.index[0]\n",
    "        index_target = target.index[0]\n",
    "        shortest_unfinished.append(int(shortest_paths[index_source][index_target]))\n",
    "    else:\n",
    "        shortest_unfinished.append(None)\n",
    "        not_found+=1\n",
    "\n",
    "unfinished_paths[\"shortest_path_length\"] = shortest_unfinished\n",
    "print(f\"{not_found} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to confirm that there are no issues in the finished paths.\n",
    "\n",
    "shortest_finished = []\n",
    "not_found2 = 0\n",
    "for i in range(len(finished_paths)):\n",
    "    source = articles.loc[articles['article'] == finished_paths.iloc[i][\"path\"][0]]\n",
    "    target = articles.loc[articles['article'] == finished_paths.iloc[i][\"path\"][-1]]\n",
    "    if len(source) != 0 and len(target) != 0:\n",
    "        index_source = source.index[0]\n",
    "        index_target = target.index[0]\n",
    "        shortest_finished.append(int(shortest_paths[index_source][index_target]))\n",
    "    else:\n",
    "        shortest_finished.append(None)\n",
    "        not_found2+=1\n",
    "\n",
    "print(f\"{not_found2} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of \"impossible\" paths\n",
    "\n",
    "# TODO: this fails since shortest_path length has not been calculated yet - move back down?\n",
    "print(f\"There are {len(finished_paths[finished_paths['shortest_path_length'] == 255])} impossible finished paths.\")\n",
    "print(f\"There are {len(unfinished_paths[unfinished_paths['shortest_path_length'] == 255])} impossible unfinished paths.\")\n",
    "\n",
    "# These will be ignored in the following analyses.\n",
    "\n",
    "# TODO: let's add some more description here what we are doing, since this is somewhat counterintuitive that there are impossbile paths\n",
    "# TODO: should we then actually exlude them by filtering the dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that the shortest path length may follow a power law. We check this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the arrays of for the cumulative distributions of in-degrees:\n",
    "finished_spl_cumulative=plt.hist(finished_paths[finished_paths['shortest_path_length'] != 255]['shortest_path_length'],bins=5,log=True,cumulative=-1,histtype='step')\n",
    "unfinished_spl_cumulative=plt.hist(unfinished_paths[unfinished_paths['shortest_path_length'] != 255]['shortest_path_length'],bins=5,log=True,cumulative=-1,histtype='step')\n",
    "plt.close()\n",
    "\n",
    "# Plotting the CCDF plots of the in-degrees for finished and unfinished paths:\n",
    "plt.loglog(finished_spl_cumulative[1][1:],finished_spl_cumulative[0], label=\"Finished paths\")\n",
    "plt.loglog(unfinished_spl_cumulative[1][1:],unfinished_spl_cumulative[0], label=\"Unfinished paths\")\n",
    "plt.title('Histogram of shortest path length (cumulative)')\n",
    "plt.ylabel('# of games (in log scale)')\n",
    "plt.xlabel('Shortest path length (in log scale)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean shortest possible paths in the finished and unfinished paths.\n",
    "\n",
    "print(\"The shortest possible paths were {:.3f} long on average in the finished paths.\".format(\n",
    "    finished_paths[finished_paths['shortest_path_length'] != 255]['shortest_path_length'].mean()\n",
    "    ))\n",
    "print(\"The shortest possible paths were {:.3f} long on average in the unfinished paths.\".format(\n",
    "    unfinished_paths[unfinished_paths['shortest_path_length'] != 255]['shortest_path_length'].mean()\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing median shortest possible paths in the finished and unfinished paths.\n",
    "\n",
    "print(\"The shortest possible paths had a median length of {:.3f} in the finished paths.\".format(\n",
    "    finished_paths[finished_paths['shortest_path_length'] != 255]['shortest_path_length'].median()\n",
    "    ))\n",
    "print(\"The shortest possible paths had a median length of {:.3f} in the unfinished paths.\".format(\n",
    "    unfinished_paths[unfinished_paths['shortest_path_length'] != 255]['shortest_path_length'].median()\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a t test on the shortest path lengths\n",
    "t_test_article_metrics([\"shortest_path_length\"], finished_paths, unfinished_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the shortest possible path lengths of finished and unfinished games is 0.0. This means we reject the null hypothesis that the shortest possible game paths are statistically the same across the two groups at the 5% level of significance, and thus the length of the shortest path possible does indeed have a statistically significant effect on whether a game will be completed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boxplot of the trends\n",
    "\n",
    "finished_shortest =  pd.DataFrame()\n",
    "finished_shortest[\"shortest_path_length\"] = finished_paths[finished_paths['shortest_path_length'] != 255][\"shortest_path_length\"]\n",
    "finished_shortest[\"path_type\"] = \"Finished paths\"\n",
    "\n",
    "unfinished_shortest =  pd.DataFrame()\n",
    "unfinished_shortest[\"shortest_path_length\"] = unfinished_paths[unfinished_paths['shortest_path_length'] != 255][\"shortest_path_length\"]\n",
    "unfinished_shortest[\"path_type\"] = \"Unfinished paths\"\n",
    "\n",
    "df_shortest = pd.concat([finished_shortest,unfinished_shortest]).reset_index(drop=True) # reset index to avoid duplicated index error from seaborn boxplot\n",
    "\n",
    "ax = sns.boxplot(x=\"path_type\", y=\"shortest_path_length\", data=df_shortest)\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\"Shortest path possible from source to target\")\n",
    "\n",
    "#TODO: Better alternative to boxplot? Currently cannot discern the 25/50/75 percentiles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot of these results provides additional context, as it makes clear that unfinished paths tend to have longer possible shortest lengths. Intuitively, it makes sense that this would be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting situation. The past two analyses show that the targets are more difficult to get to in the unfinished paths, due to their lower in-degree and the larger value of the possible shortest path to them.\n",
    "\n",
    "A challenge for us may be to try to isolate whether the difference between whether a path is finished or not can be fully explained by more objective factors like this, or if there is a human component that we can isolate as well, when controlling for factors such as these. Eg, are some categories actually more difficult to get to, or do the differences in the target category distributions in the finished and unfinished paths arise because some categories may be more likely to have longer possible shortest paths to them or have fewer links pointing at them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint1\"></a>\n",
    "\n",
    "## Checkpoint for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code saves the version of the dataframe above:\n",
    "finished_paths.to_pickle(os.path.join(GENERATED_METRICS, \"finished_paths_initial_stats.pkl\"))\n",
    "unfinished_paths.to_pickle(os.path.join(GENERATED_METRICS, \"unfinished_paths_initial_stats.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code reads that version of the dataframe from the file:\n",
    "finished_paths = pd.read_pickle(os.path.join(GENERATED_METRICS, \"finished_paths_initial_stats.pkl\"))\n",
    "unfinished_paths = pd.read_pickle(os.path.join(GENERATED_METRICS, \"unfinished_paths_initial_stats.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"everything\"></a>\n",
    "\n",
    "## 4 - Putting Everything Together\n",
    "### Building a Logistic Regression  to determine influencing factors on the propensity of a player to give up a game (restart or timeout)\n",
    "We combine all factors we have explored before to build a regression model to predict whether a player gives up to then interpret the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we first merge the article metrics (categories, word count etc.) to the finished and unfinished paths to create a dataset\n",
    "\n",
    "# merge and unfinished paths while adding a flag\n",
    "finished_paths[\"give_up\"] = 0\n",
    "unfinished_paths[\"give_up\"] = 1\n",
    "games = pd.concat((finished_paths, unfinished_paths), axis=0)\n",
    "\n",
    "# drop duplicates in the article column, since one article might belong to more than one main category, some rows are duplicated. For now, we just drop these\n",
    "article_metrics_with_categories = article_metrics_with_categories.drop_duplicates(subset=\"article\")\n",
    "\n",
    "# define columns that may be relevant from article metrics:\n",
    "# some columns are exluded since they are contained in others, or because they are complements (stopword vs non-stopword percentage)\n",
    "keep = ['article',\n",
    "        'broad_category',\n",
    "        'paragraph_count',\n",
    "        'readability_score',\n",
    "        \"stopword_percentage\",\n",
    "        'avg_word_length',\n",
    "        'avg_sent_length',\n",
    "        ]\n",
    "\n",
    "# merge on start\n",
    "start_metrics = article_metrics_with_categories[keep].add_prefix(\"start_\")\n",
    "games = pd.merge(games, start_metrics, how=\"left\", left_on=\"start\", right_on=\"start_article\") # add prefix\n",
    "print(games.shape) # check results of the merge\n",
    "\n",
    "# merge on target\n",
    "target_metrics = article_metrics_with_categories[keep].add_prefix(\"target_\")\n",
    "games = pd.merge(games, target_metrics, how=\"left\", left_on=\"target\", right_on=\"target_article\") # add prefix\n",
    "print(games.shape) # check results of the merge\n",
    "\n",
    "# subset games to only include those with reasonable path lengths - TBD, no filtering for now\n",
    "# games = games[(games.path_length > 1) & (games.path_length < 30)]\n",
    "\n",
    "\n",
    "# remove unnecessary columns\n",
    "# TODO: add backlicks to the removal column once they are in (since we do not know if there are going to be any backclicks before the game starts?)\n",
    "to_drop = ['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating',\n",
    "       'datetime', 'start', 'target', 'path_length', \"target_article\", \"start_article\", \"type\",]\n",
    "games = games.drop(to_drop, axis=1)\n",
    "print(games.shape) # check results of the subsetting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = games.isnull().sum() * 100 / len(games)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NAs - for the actual analysis we need to investigate more why these are coming from.\n",
    "# for this proof of concept we just remove them\n",
    "games = games.dropna(axis=0, how=\"any\")\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create formula for logistic regression\n",
    "target = \"give_up\"\n",
    "predictors = [col for col in games.columns if col != target]\n",
    "formula = target + \" ~ \" + \" + \".join(predictors)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# logistic regression model with the full formula (i.e. all relevant predictors)\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# regression model with limited predictors: only those visible to a player at the start of the game\n",
    "formula = 'give_up ~ links_to_target + shortest_path_length + start_broad_category + start_paragraph_count + start_readability_score + start_stopword_percentage + start_avg_sent_length + C(target_broad_category)'\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two regression models are largely congruent and offer some interesting initial findings (non-exhaustive):\n",
    "- some categories have a large and statistically significant influence on the proabability of a player giving up. A few examples:\n",
    "    - paths starting from *language and literature* or more niche topics like *Design and Technology* increase the propensity to give up\n",
    "    - a target article in the categories *Geography* or *Countries* strongly decreases the probability. This is consistent with the hypothesis that these are rather \"easy\" categories, as many links point to them.\n",
    "- similarly, many of the article related metrics are statistically significant; for instance:\n",
    "    - the shortest_path_length has a large positive coefficient, indicating that objectively longer paths do lead to more failures\n",
    "    - more detailed article metrics are statistically relevant, but the effect sizes are quite small (e.g., in-degree of target, readability score etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
