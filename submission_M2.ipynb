{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA CAPI Notebook for Project Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "1. [Loading and Preparing the Data](#load)\n",
    "    1. [Load Tabular Data](#tabu)\n",
    "    2. [Clean Tabular Data](#clean)\n",
    "2. [Data Extraction](#extract)\n",
    "    1. [Extracting metrics from textual articles](#gen)\n",
    "        1. [Defining Article Metrics](#define_arti_metrics)\n",
    "        2. [Extracting Article Metrics](#extracting_arti_metrics)\n",
    "3. [Data Exploration](#explor)\n",
    "    1. [Exploring Path Lengths](#paths)\n",
    "    2. [Exploring Categories in the Paths](#cats)\n",
    "    3. [Exploring Subject Strength in Articles](#sub)\n",
    "        1. [Exploring Subject Strength in Connected Articles](#graph_cat)\n",
    "        2. [Exploring Subject Strength in Finished Path Articles](#graph_cat_fi)\n",
    "        3. [Exploring Subject Strength in Uninished Path Articles](#graph_cat_unfi)\n",
    "4. [Data Analysis](#analysis)\n",
    "    1. [Analysing Article Metrics](#artmet)\n",
    "        1. [Analysing Article Metrics by Category](#artmet_cat)\n",
    "        2. [Analysing Article Metrics in Finished vs Unfinished paths](#artmetfu_path)\n",
    "    2. [Analysing the In-Degree of Targets in Finished vs Unfinished Paths](#ltt)\n",
    "    3. [Analysing Possible Shortest Path Distances in Finished vs Unfinished Paths](#shortest)\n",
    "    4. [Backclick Analysis](#bck_main)\n",
    "        1. [Backclicks Distribution](#bck_distr)\n",
    "        2. [Backclick Frequency in Finished vs. Unfinished Games](#bck-freq-compare)\n",
    "        3. [Backclick Rate per Category](#bck-rate-cat)\n",
    "        4. [Backclick Frequency Impact on Finishing the Game Likelihood](#bck-test)\n",
    "5. [Putting Everything Together](#everything)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats \n",
    "\n",
    "# Helper functions from utils folder\n",
    "from utils.analysis import t_test_article_metrics, visualize_article_connections_per_category, sorted_category_counts, shortest_path_find, simple_t_test\n",
    "from utils.preprocessing import (merge_articles_categories, create_category_dictionaries,\n",
    "                                filter_games, get_backclicked_pages)\n",
    "\n",
    "# Formatting libraries\n",
    "import urllib\n",
    "import datetime as datetime\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "default_colors = mpl.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Imports to perform article analysis\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt') # Punkt tokenizer\n",
    "nltk.download('stopwords') # Common stopwords\n",
    "\n",
    "# Regression libraries\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Stats libraries\n",
    "from statistics import mean, median\n",
    "\n",
    "# Load config and extract variables\n",
    "import config\n",
    "DATA_PATH = config.PATH_TO_DATA\n",
    "PATH_GRAPH_FOLDER = config.PATH_GRAPH_FOLDER\n",
    "ARTICLE_FOLDER = config.ARTICLE_FOLDER\n",
    "GENERATED_METRICS = config.GENERATED_METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 1 - Loading and Preparing the Data\n",
    "\n",
    "Note that you can load the data from [here](#checkpoint1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabu\"></a>\n",
    "#### 1.1 - Load Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all data (except wikipedia articles)\n",
    "finished_paths = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"paths_finished.tsv\"), sep='\\t', skiprows=15, \n",
    "                             names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"rating\"])\n",
    "unfinished_paths = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"paths_unfinished.tsv\"), sep='\\t', skiprows=16, \n",
    "                               names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"target\", \"type\"])\n",
    "edges = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"links.tsv\"), sep='\\t', skiprows=15, names=[\"start\", \"end\"], encoding=\"utf-8\")\n",
    "articles = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"articles.tsv\"), sep='\\t', skiprows=12, names=[\"article\"], encoding=\"utf-8\")\n",
    "categories = pd.read_csv(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"categories.tsv\"), sep='\\t', skiprows=13, \n",
    "                         names=[\"article\", \"category\"], encoding=\"utf-8\")\n",
    "shortest_paths = np.genfromtxt(os.path.join(DATA_PATH, PATH_GRAPH_FOLDER, \"shortest-path-distance-matrix.txt\"), delimiter=1, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "#### 1.2 - Clean Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding in edge list\n",
    "display(edges.head())\n",
    "edges[\"start\"] = edges.start.apply(urllib.parse.unquote)\n",
    "edges[\"end\"] = edges.end.apply(urllib.parse.unquote)\n",
    "display(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datetime as datetime object\n",
    "finished_paths[\"datetime\"] = finished_paths.timestamp.apply(datetime.datetime.fromtimestamp)\n",
    "unfinished_paths[\"datetime\"] = unfinished_paths.timestamp.apply(datetime.datetime.fromtimestamp)\n",
    "display(unfinished_paths.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for articles\n",
    "display(articles.head())\n",
    "articles[\"article\"] = articles.article.apply(urllib.parse.unquote)\n",
    "display(articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up url encoding for categories\n",
    "display(categories.head())\n",
    "categories[\"article\"] = categories.article.apply(urllib.parse.unquote)\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify broad categories of articles\n",
    "display(categories.head())\n",
    "categories[\"broad_category\"] = categories[\"category\"].apply(lambda x: x.split(\".\")[1]) # first entry after subject.\n",
    "display(categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge articles and categories\n",
    "articles_categories = pd.merge(articles, categories, how=\"left\", on=\"article\")\n",
    "display(articles_categories.head())\n",
    "\n",
    "# 6 articles without category!\n",
    "print(\"Merge introduced {} NAs in category columns:\".format(articles_categories.category.isna().sum()))\n",
    "articles_categories[articles_categories.category.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paths to a readable format (lists) and remove url encoding\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "finished_paths[\"path\"] = finished_paths[\"path\"].apply(lambda x: [urllib.parse.unquote(y) for y in x])\n",
    "\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "unfinished_paths[\"path\"] = unfinished_paths[\"path\"].apply(lambda x: [urllib.parse.unquote(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and target articles of path\n",
    "finished_paths[\"start\"] = [path[0] for path in finished_paths[\"path\"]]\n",
    "finished_paths[\"target\"] = [path[-1] for path in finished_paths[\"path\"]]\n",
    "\n",
    "unfinished_paths[\"start\"] = [path[0] for path in unfinished_paths[\"path\"]]\n",
    "unfinished_paths[\"target\"] = unfinished_paths[\"target\"].apply(urllib.parse.unquote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extract\"></a>\n",
    "\n",
    "## 2 - Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gen\"></a>\n",
    "\n",
    "#### 2.1 - Extracting metrics from textual articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_arti_metrics\"></a>\n",
    "##### 2.1.1 - Defining Article Metrics\n",
    "\n",
    "The following metrics are extracted by performing textual pre-processing techniques in the wikipedia articles:\n",
    "* Total word count: To understand the length of the article.\n",
    "* Non stopword frequency: To identify words that contribute to the content's meaning.\n",
    "* Stopword frequency: To identify common words that may not contribute to the content's meaning.\n",
    "* Average word length: To assess the complexity of the language used.\n",
    "* Average sentence length: Longer or more complex sentences (based on characters) may contribute to frustration.\n",
    "* Number of paragraphs: To see if the article's structure plays a role in people giving up.\n",
    "* Keyword frequency: To identify the most common keywords to understand the article's focus.\n",
    "* Readability: To see if the ease of reading the article has an impact (metric: Flesch Reading Ease Score) Link: https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_article(article_text):\n",
    "    preprocessed_text = article_text\n",
    "    preprocessed_text = preprocessed_text.lower()\n",
    "    preprocessed_text = preprocessed_text.replace(\"\\n   \", \" \") # As the articles are not continuous sentences\n",
    "    return preprocessed_text\n",
    "\n",
    "def calculate_article_metrics(article_text):\n",
    "    preprocessed_text = proprocess_article(article_text)\n",
    "\n",
    "    words = word_tokenize(preprocessed_text)\n",
    "    sentences = sent_tokenize(preprocessed_text)\n",
    "\n",
    "    # Calculate total word count\n",
    "    total_word_count = len(words)\n",
    "\n",
    "    # Calculate stopword frequency\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stopwords_count = 0\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word.isalpha() and word.lower() in stop_words:\n",
    "            stopwords_count +=1\n",
    "        if word.isalpha() and word.lower() not in stop_words:\n",
    "            unique_words.append(word.lower())\n",
    "\n",
    "    # Calculate average word length\n",
    "    average_word_length = sum(len(word) for word in words) / total_word_count\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    average_sentence_length = sum(len(sentence) for sentence in sentences) / len(sentences)\n",
    "\n",
    "    # Calculate number of paragraphs (assume every new line \\n is paragraph)\n",
    "    paragraphs_count = preprocessed_text.count('\\n') + 1 # Count last paragraph\n",
    "\n",
    "    # Calculate keyword frequency\n",
    "    word_freq = nltk.FreqDist(unique_words)\n",
    "    most_common_words = word_freq.most_common(10)  # Parameter to adjust\n",
    "\n",
    "    # Calculate readability (Flesch Reading Ease Score) - 100: Easy to read, 0: Very confusing\n",
    "    readability = textstat.flesch_reading_ease(preprocessed_text)\n",
    "\n",
    "    return {\n",
    "        \"word_count\": total_word_count,\n",
    "        \"non_stopword_count\": total_word_count - stopwords_count,\n",
    "        \"non_stopword_percentage\": (total_word_count - stopwords_count) / total_word_count,\n",
    "        \"stopword_count\": stopwords_count,\n",
    "        \"stopword_percentage\": stopwords_count / total_word_count,\n",
    "        \"avg_word_length\": average_word_length,\n",
    "        \"avg_sent_length\": average_sentence_length,\n",
    "        \"paragraph_count\": paragraphs_count,\n",
    "        \"common_words\": most_common_words,\n",
    "        \"readability_score\": readability,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extracting_arti_metrics\"></a>\n",
    "##### 2.1.2 - Extracting Article Metrics\n",
    "\n",
    "The commented code below was used to access the `plaintext_articles` folder and read all articles inside, creating a dataframe with all the metric information (see table below). To reduce runtime, we compute the article metrics once and then read the generated CSV file.\n",
    "\n",
    "\n",
    "Article Metrics DataFrame Description:\n",
    "| Metric             | Metric Name                  | Description                                                                                                   |\n",
    "|-------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| `word_count`            | Total Word Count        | Represents the total word count in the article.                                                                |\n",
    "| `stopword_count`        | Stopword Count          | Measures the number of words that do not contribute significantly to the content's meaning.                 |\n",
    "| `stopword_percentage`   | Stopword Frequency      | Measures the percentage of words that do not contribute significantly to the content's meaning.                 |\n",
    "| `non_stopword_count`    | Non-Stopword Count      | Measures the number of words that contribute to the content's meaning.                              |\n",
    "| `non_stopword_percentage`| Non-Stopword Frequency  | Measures the percentage of words that contribute to the content's meaning.                              |\n",
    "| `avg_word_length`       | Average Word Length     | Calculates the average word length (in characters) in the article.                                               |\n",
    "| `avg_sent_length`       | Average Sentence Length | Calculates the average sentence length (in characters) in the article.                                               |\n",
    "| `paragraph_count`       | Number of Paragraphs    | Calculates the total number of paragraphs in the article.                                                        |\n",
    "| `common_words`          | Keyword Frequency       | Extracts the 10 most frequent of keywords.                  |\n",
    "| `readability_score`     | Readability (Flesch Score)| Utilizes the Flesch Reading Ease Score for assessing readability. [Learn more](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "folder_path = os.path.join(DATA_PATH, ARTICLE_FOLDER)\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "\n",
    "  article_metrics = pd.DataFrame(columns=[\"article\", \"word_count\", \"non_stopword_count\", \"non_stopword_percentage\", \"stopword_count\", \"stopword_percentage\", \"avg_word_length\", \"avg_sent_length\", \"paragraph_count\", \"common_words\", \"readability_score\"])\n",
    "\n",
    "  for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "      root, extension = os.path.splitext(file_name)\n",
    "      readable_file_name = urllib.parse.unquote(root)\n",
    "      \n",
    "      with open(file_path, \"r\", encoding=\"utf-8\") as article:\n",
    "        metrics = calculate_article_metrics(article.read())\n",
    "\n",
    "        metrics[\"article\"] = readable_file_name\n",
    "        article_metrics.loc[len(article_metrics)] = metrics\n",
    "else:\n",
    "  raise FileNotFoundError(\"The specified folder path does not exist or is not a directory.\")\n",
    "\n",
    "article_metrics.to_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"), index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the article data (as mentioned before, this is done to reduce runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_metrics = pd.read_csv(os.path.join(GENERATED_METRICS, \"article_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(article_metrics.info())\n",
    "display(article_metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explor\"></a>\n",
    "\n",
    "## 3 -Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths\"></a>\n",
    "\n",
    "#### 3.1 - Exploring Path Lengths\n",
    "\n",
    "Compare the path lengths between the finished and unfinished paths to detect potential outliers or trends that might influence the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate path lengths for finished paths and show summary statistics\n",
    "finished_paths[\"path_length\"] = finished_paths.path.apply(lambda el: len(el))\n",
    "finished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate path lengths for unfinished paths and show summary statistics\n",
    "unfinished_paths[\"path_length\"] = unfinished_paths.path.apply(lambda el: len(el))\n",
    "unfinished_paths[\"path_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distributions of finished and unfinished paths\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "threshold = 40 # for now we remove some outliers to make the plots meaningful\n",
    "\n",
    "# note, plotting the density to compare the distributions (meaningful 0 length and 1 length paths?)\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "sns.histplot(x=finished_paths.path_length[finished_paths.path_length < threshold], ax=ax, discrete=True, alpha=0.4, label=\"finished paths\", stat=\"density\", color=default_colors[0])\n",
    "sns.histplot(x=unfinished_paths.path_length[unfinished_paths.path_length < threshold], ax=ax, discrete=True, alpha=0.4, label=\"unfinished paths\", stat=\"density\", color=default_colors[1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of path lengths across finished, restarted paths and unfinished paths that timed out\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "sns.histplot(x=finished_paths.path_length[finished_paths.path_length < threshold], ax=axes[0], discrete=True, color=default_colors[0])\n",
    "axes[0].set_title(\"Finished Paths\")\n",
    "\n",
    "unfinished_restart= unfinished_paths[(unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"restart\")]\n",
    "sns.histplot(data=unfinished_restart, x=\"path_length\", ax=axes[1], discrete=True, color=default_colors[0])\n",
    "axes[1].set_title(\"Unfinished Paths - Restart\")\n",
    "\n",
    "unfinished_timeout = unfinished_paths[(unfinished_paths.path_length < threshold) & (unfinished_paths.type == \"timeout\")]\n",
    "sns.histplot(data=unfinished_timeout, x=\"path_length\", ax=axes[2], discrete=True, color=default_colors[0])\n",
    "axes[2].set_title(\"Unfinished Paths - Timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Important note: unfinished paths before 2011 are missing\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "filter = lambda df, col: df[col]\n",
    "sns.histplot(x=filter(finished_paths, \"datetime\"), ax=ax, alpha=0.4, label=\"finished\", bins=100, color=default_colors[0])\n",
    "sns.histplot(x=filter(unfinished_paths, \"datetime\"), ax=ax, alpha=0.4, label=\"unfinished\", bins=50, color=default_colors[1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Unfinished paths before 2011 are not present in the dataset. It is crucial to keep this in mind for subsequent analyses on differences between finished and unfinished paths, since in some cases the results could change if we include all the paths or we exclude those before 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct the duration in seconds\n",
    "# since the timeout is after 1800 seconds of inactivity, we consider here the correct duration\n",
    "unfinished_timeout[\"effectiveDuration\"] = unfinished_timeout[\"durationInSec\"] - 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare the distributions of game duration\n",
    "# different distribution for finished, restart and timeout\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6))\n",
    "ax.set_title(\"Distribution of Finished vs. Unfinished Paths\")\n",
    "filter = lambda df, col: df[(df[col] >= 0) & (df[col] < 1000)][col] # chose 1000 for a better visualization\n",
    "sns.histplot(x=filter(finished_paths, \"durationInSec\"), ax=ax, alpha=0.4, label=\"finished\", stat=\"density\", bins=100, color=default_colors[0])\n",
    "sns.histplot(x=filter(unfinished_restart[unfinished_restart.path_length > 1], \"durationInSec\"), ax=ax, alpha=0.4, label=\"restart\", stat=\"density\", bins=100, color=default_colors[1])\n",
    "sns.histplot(x=filter(unfinished_timeout[unfinished_timeout.path_length > 1], \"effectiveDuration\"), ax=ax, alpha=0.4, label=\"timeout\", stat=\"density\", bins=100, color=default_colors[2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that we have some weird values for `durationInSec`. Further esploration might be appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{} paths are labeled as 'timeout' but their duration is below 1800 seconds\".format(len(unfinished_timeout[\"durationInSec\"][unfinished_timeout[\"durationInSec\"] < 1800])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note also that there are few articles that are very often chosen as targets (or as start articles). \n",
    "Also this could influence further analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print frequency articles\n",
    "# this is computed with respect to all (finished and unfinished) paths\n",
    "target_counts = (unfinished_restart[\"target\"].value_counts() + finished_paths[\"target\"].value_counts() + unfinished_timeout[\"target\"].value_counts()).sort_values(ascending=False)\n",
    "\n",
    "target_percentages = target_counts / target_counts.sum()\n",
    "\n",
    "print(\"The 10 most frequent targets (frequency)\")\n",
    "display(target_percentages.head(10))\n",
    "\n",
    "print(\"The 10 least frequent targets (frequency)\")\n",
    "display(target_percentages.sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print frequency articles for start\n",
    "# this is computed with respect to all (finished and unfinished) paths\n",
    "start_counts = (unfinished_restart[\"start\"].value_counts() + finished_paths[\"start\"].value_counts() + unfinished_timeout[\"start\"].value_counts()).sort_values(ascending=False)\n",
    "\n",
    "start_percentages = start_counts / start_counts.sum()\n",
    "\n",
    "print(\"The 10 most frequent targets (frequency)\")\n",
    "display(start_percentages.head(10))\n",
    "\n",
    "print(\"The 10 least frequent targets (frequency)\")\n",
    "display(start_percentages.sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Question: why few articles occur very frequently?\n",
    "- For targets: 5 articles constitute the 14.20% of the total targets\n",
    "- For starting articles: 5 articles constitute the 12.82% of the total starting articles\n",
    "\n",
    "Also this requires further checks and deeper analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are quite a few outliers and extreme values for path lengths. For the future analysis we need to consider:\n",
    "- Games with a path length of 1.\n",
    "- Games with high path length (max. path length=435).\n",
    "\n",
    "We plan to address these questions in the analysis, e.g., through a sensitivity analysis: We run our analysis first on the regular data, before checking if we get similar results while removing certain outlier games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cats\"></a>\n",
    "\n",
    "#### 3.2 - Exploring Categories in the Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the occurances of categories in the paths, to gain an understanding of whether certain categories lead to games that are on average easier for people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing which categories are most represented in articles\n",
    "count_articles = categories.groupby(\"broad_category\").size()\n",
    "\n",
    "print(\"Below shows how many articles each of the broad categories are represented by\")\n",
    "display(count_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for easy discovery of what categories an article belongs to\n",
    "article_to_category, article_to_broad_category = create_category_dictionaries(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each category has occured as a target in the finished and unfinished paths.\n",
    "# Note that some articles are represented by multiple categories, which are thus counted extra.\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories in the finished paths\n",
    "sorted_cats_f = sorted_category_counts(finished_paths, article_to_broad_category)\n",
    "\n",
    "# Creating a dictionary of counts of the broad categories in the unfinished paths\n",
    "sorted_cats_u = sorted_category_counts(unfinished_paths, article_to_broad_category)\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(list(sorted_cats_f.keys()), sorted_cats_f.values(), label=\"Finished paths\", color=default_colors[0])\n",
    "ax2 = plt.barh(list(sorted_cats_u.keys()), sorted_cats_u.values(), left=list(sorted_cats_f.values()), label=\"Unfinished paths\", color=default_colors[1])\n",
    "plt.xlabel(\"Count\")\n",
    "plt.title(\"Occurences of categories as targets\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows clearly that some categories occur as targets relatively more in finished paths while others have the opposite outcome. For example, \"Couuntries\" occurs as a target in finished paths multiple times as often as it does in unfinished paths, whereas \"Everyday_life\" occurs as a target in finished paths only slightly more often than it does in unfinished paths. \n",
    "\n",
    "The plot, however, also shows the imbalance in the categories. We have a lot of paths ending in \"Geography\" and \"Science\", but very few ending in \"Mathematics\" and \"Art\". We can create a plot with better interpretability regarding the influence of categories by charting the empirical likelihood of a target belonging to a certain category not being reached. This is the probability of a game being unfinished ($u$) for a given category $i$, and is calculated as:\n",
    "\n",
    "$ P(u|i) = \\frac{c^u_i}{c^u_i + c^f_i} $\n",
    "\n",
    "\n",
    "where $c^u_i$ is the count of category i occuring as a target in the unfinished paths, whereas $c^f_i$ is the count of category i occuring as a target in the finished paths.\n",
    "\n",
    "However, a large part of the imbalance in the number of finished and unfinished paths comes from the fact that data on unfinished paths started to be collected only a couple of years after it did for finished paths. Thus, to have a probability value that makes sense, we need to exclude the finished path data from the time before data for unfinished paths also began to be collected. We will only perform this filtering for this particular analysis, however, as it results in a significant loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of counts of the broad categories in the finished paths played after data began to be collected for unfinished paths.\n",
    "\n",
    "finished_paths_post2011 = finished_paths[finished_paths[\"datetime\"] >= unfinished_paths[\"datetime\"].min()]\n",
    "sorted_cats_f_p2011 = sorted_category_counts(finished_paths_post2011, article_to_broad_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the empirical likelihood that a target belonging to a certain category not being reached.\n",
    "cat_unfinished_prob = {}\n",
    "for i in sorted_cats_f:\n",
    "    cat_unfinished_prob[i] = sorted_cats_u[i]/(sorted_cats_u[i]+sorted_cats_f_p2011[i])\n",
    "\n",
    "# Plotting the results.\n",
    "ax = plt.barh(list(cat_unfinished_prob.keys()), cat_unfinished_prob.values(), color=default_colors[0])\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.title(\"Empirical likelihood of not reaching certain categories\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reinforces the possibility that certain categories make for easier games, as we can see that people are empirically more likely to not reach targets belonging to certain categories more than others.. Nonetheless, to establish a proper relationship, we may in the future need to control for certain other variables. For example, it could be that articles in the \"Countries\" category are simply better connected than those in the \"Everyday_life\" category. For this, we would need to potentially make use of matching and propensity scores to conduct a proper causal analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sub\"></a>\n",
    "\n",
    "#### 3.3 - Exploring Subject Strength in Articles\n",
    "\n",
    "Conducting an exploratory analysis on the relationship between categories of articles, our focus encompasses both neighboring articles (i.e., those directly connected by a link) and start/target articles in both finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 6 articles without category (found in section 1.2)!\n",
    "articles_categories = articles_categories.dropna(subset=[\"broad_category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat\"></a>\n",
    "\n",
    "##### 3.3.1 - Exploring Subject Strength in Connected Articles\n",
    "Visualizing the strength of the categories for connected articles (those which are connected by an edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the edges dataframe\n",
    "edge_category = merge_articles_categories(edges, [\"start\", \"end\"], articles_categories)\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "visualize_article_connections_per_category(edge_category, articles_categories, \"Article Connections Based on Category (Normalized and Scaled Edges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_fi\"></a>\n",
    "\n",
    "##### 3.3.2 - Exploring Subject Strength in Finished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the finished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the finished_paths dataframe\n",
    "finished_paths_categories = merge_articles_categories(finished_paths, [\"start\", \"target\"], articles_categories)\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "visualize_article_connections_per_category(finished_paths_categories, articles_categories, \"Start & Target Article Connections in Finished Path Based on Category (Normalized and Scaled Edges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"graph_cat_unfi\"></a>\n",
    "\n",
    "##### 3.3.3 - Exploring Subject Strength in Uninished Path Articles\n",
    "Visualizing the strength of the categories for both start and target articles in the unfinished paths using a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the category information to the articles in the unfinished_paths dataframe\n",
    "unfinished_paths_categories = merge_articles_categories(unfinished_paths, [\"start\", \"target\"], articles_categories)\n",
    "\n",
    "# Visualizing their article relations through categories in a graph\n",
    "visualize_article_connections_per_category(unfinished_paths_categories, articles_categories, \"Start & Target Article Connections in Unfinished Path Based on Category (Normalized and Scaled Edges)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "\n",
    "## 4 - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet\"></a>\n",
    "\n",
    "#### 4.1 - Analysing Article Metrics\n",
    "\n",
    "Conducting an analysis on the article metrics extracted in section 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmet_cat\"></a>\n",
    "\n",
    "##### 4.1.1 - Analysing Article Metrics by Category\n",
    "\n",
    "Viewing how the different metrics (e.g., word_count, stopword_count, etc.) vary across different categories of articles. These differences are evident by visualizing the metrics in both bar plots and violin plots (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge articles with their corresponding categories\n",
    "article_metrics_with_categories = article_metrics.merge(categories, how=\"left\", on=[\"article\"])\n",
    "display(article_metrics_with_categories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = ['word_count', 'stopword_count', 'stopword_percentage', 'non_stopword_count', 'non_stopword_percentage','avg_word_length', 'avg_sent_length', 'paragraph_count','readability_score']\n",
    "fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "  # Bar plot\n",
    "  ax_bar = axes[idx, 0]\n",
    "  sns.barplot(x=article_metrics_with_categories[\"broad_category\"], y=article_metrics_with_categories[metric], errorbar=(\"ci\", 95), ax=ax_bar, palette=default_colors)\n",
    "  ax_bar.set_xlabel(\"Category\")\n",
    "  ax_bar.set_ylabel(metric)\n",
    "  ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "  ax_bar.set_xticklabels(ax_bar.get_xticklabels(), rotation=90)\n",
    "\n",
    "  # Violin plot\n",
    "  ax_violin = axes[idx, 1]\n",
    "  sns.violinplot(x=article_metrics_with_categories[\"broad_category\"], y=article_metrics_with_categories[metric], ax=ax_violin, palette=default_colors)\n",
    "  ax_violin.set_xlabel(\"Category\")\n",
    "  ax_violin.set_ylabel(metric)\n",
    "  ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "  ax_violin.set_xticklabels(ax_violin.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* Interesting observations from the plot reveal that science articles generally tend to be shorter in length, evident in lower word counts and paragraph counts.\n",
    "* Mathematical article appear to be the least consistent in word length, with a notable high variance.\n",
    "* Articles related to countries, on average, display the lowest proportion of stopwords (percentage-wise), suggesting they might posses a rich vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artmetfu_path\"></a>\n",
    "\n",
    "##### 4.1.2 - Analysing Article Metrics in Finished vs Unfinished paths\n",
    "\n",
    "Viewing how the different metrics (e.g., word_count, stopword_count, etc.) vary across start/target articles in finished/unfinished paths. Both bar plots and violin plots are used to understand the distribution our these metrics. We observe remarkable similarity accross all plots, but minor differences are apparent in word count and paragraph count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the article metrics per finished and unfinished parths (both for start and end articles)\n",
    "start_finished_article_metrics = finished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\")\n",
    "end_finished_article_metrics = finished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\")\n",
    "start_unfinished_article_metrics = unfinished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"start\", right_on=\"article\")\n",
    "end_unfinished_article_metrics = unfinished_paths.merge(article_metrics_with_categories, how=\"left\", left_on=\"target\", right_on=\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\"word_count\", \"stopword_count\", \"stopword_percentage\", \"non_stopword_count\", \"non_stopword_percentage\",\"avg_word_length\", \"avg_sent_length\", \"paragraph_count\", \"readability_score\"]\n",
    "dataframes = [start_finished_article_metrics, start_unfinished_article_metrics, end_finished_article_metrics, end_unfinished_article_metrics]\n",
    "dataframe_labels = [\"Start Finished\", \"Start Unfinished\", \"Target Finished\", \"Target Unfinished\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=2, figsize=(15, 6 * len(metrics_to_plot)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "  data = [df[metric] for df in dataframes]\n",
    "  \n",
    "  # Bar plot\n",
    "  ax_bar = axes[idx, 0]\n",
    "  sns.barplot(data=data, errorbar=(\"ci\", 95), ax=ax_bar, palette=default_colors)\n",
    "  ax_bar.set_xlabel(\"Type of article\")\n",
    "  ax_bar.set_ylabel(metric)\n",
    "  ax_bar.set_title(\"Mean and CI of {} per Category\".format(metric))\n",
    "  ax_bar.set_xticklabels(dataframe_labels)\n",
    "\n",
    "  # Violin plot\n",
    "  ax_violin = axes[idx, 1]\n",
    "  sns.violinplot(data=data, ax=ax_violin, palette=default_colors)\n",
    "  ax_bar.set_xlabel(\"Type of article\")\n",
    "  ax_violin.set_ylabel(metric)\n",
    "  ax_violin.set_title(\"Distribution of {} per Category\".format(metric))\n",
    "  ax_violin.set_xticklabels(dataframe_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing article metric differences in starting articles in finished/unfinished paths.\n",
    "print(\"Start Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_finished_article_metrics, start_unfinished_article_metrics)\n",
    "\n",
    "# Comparing article metric differences in target articles in finished/unfinished paths.\n",
    "print(\"\\nTarget Articles (comparing finished vs unfinished):\")\n",
    "t_test_article_metrics(metrics_to_plot, end_finished_article_metrics, end_unfinished_article_metrics)\n",
    "\n",
    "\n",
    "# Comparing article metric differences in (start, target) article pairs in finished/unfinished paths.\n",
    "# This enables us to identify differences between the articles one starts with and those that need to be finished \n",
    "# in both finished/unfinished paths, aiming to uncover potential factors that might impact the player.\n",
    "print(\"\\nFinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_finished_article_metrics, end_finished_article_metrics)\n",
    "\n",
    "print(\"\\nUnfinished Articles (comparing start vs target):\")\n",
    "t_test_article_metrics(metrics_to_plot, start_unfinished_article_metrics, end_unfinished_article_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics                    | Start Articles (Finished vs Unfinished) | Target Articles (Finished vs Unfinished) | Finished Articles (Start vs Target) | Unfinished Articles (Start vs Target) |\n",
    "|----------------------------|-----------------------------------------|------------------------------------------|--------------------------------------|----------------------------------------|\n",
    "| word_count                  | t-statistic: 1.873, p-value: 0.061       | t-statistic: 36.838, p-value: 0.000      | t-statistic: -30.824, p-value: 0.000 | t-statistic: 8.949, p-value: 0.000     |\n",
    "| stopword_count              | t-statistic: 1.353, p-value: 0.176       | t-statistic: 34.692, p-value: 0.000      | t-statistic: -30.009, p-value: 0.000 | t-statistic: 8.076, p-value: 0.000     |\n",
    "| stopword_percentage         | t-statistic: -3.366, p-value: 0.001      | t-statistic: 4.045, p-value: 0.000       | t-statistic: -0.362, p-value: 0.717 | t-statistic: 6.045, p-value: 0.000     |\n",
    "| non_stopword_count          | t-statistic: 2.131, p-value: 0.033       | t-statistic: 37.599, p-value: 0.000      | t-statistic: -30.943, p-value: 0.000 | t-statistic: 9.317, p-value: 0.000     |\n",
    "| non_stopword_percentage     | t-statistic: 3.366, p-value: 0.001       | t-statistic: -4.045, p-value: 0.000      | t-statistic: 0.362, p-value: 0.717  | t-statistic: -6.045, p-value: 0.000    |\n",
    "| avg_word_length             | t-statistic: -3.090, p-value: 0.002      | t-statistic: 10.974, p-value: 0.000     | t-statistic: 2.987, p-value: 0.003  | t-statistic: 14.113, p-value: 0.000   |\n",
    "| avg_sent_length             | t-statistic: 4.863, p-value: 0.000       | t-statistic: -0.260, p-value: 0.795     | t-statistic: -3.443, p-value: 0.001 | t-statistic: -6.964, p-value: 0.000    |\n",
    "| paragraph_count             | t-statistic: 0.038, p-value: 0.970       | t-statistic: 37.247, p-value: 0.000     | t-statistic: -29.294, p-value: 0.000| t-statistic: 11.952, p-value: 0.000   |\n",
    "| readability_score           | t-statistic: 4.652, p-value: 0.000       | t-statistic: -21.100, p-value: 0.000   | t-statistic: -8.953, p-value: 0.000 | t-statistic: -27.779, p-value: 0.000 |\n",
    "\n",
    "1. **Finished vs Unfinished Start Articles:**\n",
    "   - The stopword_percentage is significantly lower (and non_stopword_percentage higher) in finished articles than unfinished, suggesting a potential emphasis on more meaningful content. \n",
    "   - Finished start articles also tend to have higher avg_sent_length and readability_score, indicating a focus on well-structured and reader-friendly content.\n",
    "\n",
    "2. **Finished vs Unfinished Target Articles:**\n",
    "   - Finished target articles exhibit significantly higher values across various metrics, including word_count, stopword_percentage (with lower non_stopword_percentage), avg_word_length, and paragraph_count. \n",
    "   - Additionally, they have a significantly lower readability_score, suggesting that finished target articles could be more challenging to comprehend.\n",
    "\n",
    "3. **Finished Articles (Start, Target):**\n",
    "   - When comparing finished start and target articles, strong differences emerge in various metrics. \n",
    "   - Finished start articles tend to be shorter with lower word_count and paragraph counts, but have a slightly higher readability_score.\n",
    "\n",
    "4. **Unfinished Articles (Start, Target):**\n",
    "   - Starting articles exhibit significantly higher values accross all metrics, except avg_sent_length, non_stopword_percentage, and readability_score. \n",
    "   - This implies that target articles which players failed to complete, are consistently longer, include more stopword_percentage (therefore have a lower non_stopword_percentage) and have higher avg_word_length. \n",
    "   - Interestingly, unfinished start/target articles exhibit the most significant difference in readability scores, prompting us to consider whether this larger gap affects the user's ability to finish the game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ltt\"></a>\n",
    "\n",
    "#### 4.2 - Analysing the In-Degree of Targets in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that certain paths are easier objectively because their targets have a larger \"in-degree\", i.e. the number of edges in the graph pointing to it. This would be intuitive: if there are more ways to get to the target, it should be easier to do so. This section explores whether this idea is reflected in the distributions of the in-degrees of the targets in finished and unfinished paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many links point to targets in finished and unfinished paths, known as the \"in-degree\".\n",
    "\n",
    "finished_paths[\"links_to_target\"] = finished_paths[\"path\"].apply(lambda x: len(edges.loc[edges[\"end\"] == x[-1]]))\n",
    "unfinished_paths[\"links_to_target\"] = unfinished_paths[\"target\"].apply(lambda x: len(edges.loc[edges[\"end\"] == x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for targets with an in-degree of 0.\n",
    "\n",
    "zeros_finished = [1 for x in finished_paths['links_to_target'] if x == 0]\n",
    "zeros_unfinished = [1 for x in unfinished_paths['links_to_target'] if x == 0]\n",
    "\n",
    "print(\"There were {} targets with an in-degree of 0 in the finished paths.\".format(sum(zeros_finished)))\n",
    "print(\"There were {} targets with an in-degree of 0 in the unfinished paths.\".format(sum(zeros_unfinished)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unreasonable that the targets have an in-degree of 0. It is likely that these outcomes occur because the information provided in the data was not updated after the wikipedia graph kept evolving, as one target with 0 in-degree was reached. It is nonetheless believed that the information is accurate in the majority of cases, so we simply ignore the targets with 0 in-degree in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the data below.\n",
    "\n",
    "finished_in_degree = [x for x in finished_paths['links_to_target'] if x != 0]\n",
    "unfinished_in_degree = [x for x in unfinished_paths['links_to_target'] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(finished_in_degree, bins=50)\n",
    "ax1.set_xlabel(\"in-degree\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of in-degree of targets in finished paths\")\n",
    "\n",
    "ax2.hist(unfinished_in_degree, bins=50)\n",
    "ax2.set_xlabel(\"in-degree\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of in-degree of targets in unfinished paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above show that the data is clearly skewed heavily to the right. We suspect this may be a power law. We check this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the arrays of for the cumulative distributions of in-degrees:\n",
    "finished_indegree_cumulative=plt.hist(finished_paths.links_to_target, bins=100, log=True, cumulative=-1, histtype='step', color=default_colors[0])\n",
    "unfinished_indegree_cumulative=plt.hist(unfinished_paths.links_to_target, bins=100, log=True, cumulative=-1, histtype='step', color=default_colors[1])\n",
    "plt.close()\n",
    "\n",
    "# Plotting the CCDF plots of the in-degrees for finished and unfinished paths:\n",
    "plt.loglog(finished_indegree_cumulative[1][1:], finished_indegree_cumulative[0], label=\"Finished paths\")\n",
    "plt.loglog(unfinished_indegree_cumulative[1][1:], unfinished_indegree_cumulative[0], label=\"Unfinished paths\")\n",
    "plt.title('CCDF plot of the in-degree of targets')\n",
    "plt.ylabel('# of targets (in log scale)')\n",
    "plt.xlabel('In-degree (in log scale)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that the in-degree may not necessarily follow a power law, as the CCDF plot does not have a negative linear slope. We thus try to see if the data is log-normal, be plotting the histogram of the logarithms of the in-degrees of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the logs of the data below.\n",
    "\n",
    "logged_finished = [np.log(x) for x in finished_paths['links_to_target'] if x != 0]\n",
    "logged_unfinished = [np.log(x) for x in unfinished_paths['links_to_target'] if x != 0]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(logged_finished, bins=8, color=default_colors[0])\n",
    "ax1.set_xlabel(\"log(in-degree)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Histogram of in-degree of targets in finished paths\")\n",
    "\n",
    "ax2.hist(logged_unfinished, bins=8, color=default_colors[0])\n",
    "ax2.set_xlabel(\"log(in-degree)\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Histogram of in-degree of targets in unfinished paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show that the data may indeed be distributed log-normally. This is why we report both the mean and the median values for these distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean in-degree of the targets in the finished and unfinished paths.\n",
    "print(\"The targets that were reached had an in-degree of {:.3f} on average.\".format(mean(finished_in_degree)))\n",
    "print(\"The targets that were not reached had an in-degree of {:.3f} on average.\".format(mean(unfinished_in_degree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing median in-degree of the targets in the finished and unfinished paths.\n",
    "print(\"The targets that were reached had a median in-degree of {}.\".format(median(finished_in_degree)))\n",
    "print(\"The targets that were not reached had a median in-degree of {}.\".format(median(unfinished_in_degree)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a t-test assums a normal distribution around the mean, we cannot do the t-test for the in-degrees directly, which are heavily skewed. Instead, we do it for the logarithms of the in-degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting a t-test for the in-degree of targets for finished and unfinished paths.\n",
    "simple_t_test(logged_finished, logged_unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the number of links pointing to the targets of finished and unfinished paths is 0.0. This means we reject the null hypothesis that the number of links pointing to the targets are statistically the same at the 5% level of significance, indicating that the in-degree of the target indeed may have a statistical significance in whether a game will be finished or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a box plot of the trends.\n",
    "\n",
    "finished_links =  pd.DataFrame()\n",
    "finished_links[\"links_to_target\"] = finished_in_degree\n",
    "finished_links[\"path_type\"] = \"Finished paths\"\n",
    "\n",
    "unfinished_links =  pd.DataFrame()\n",
    "unfinished_links[\"links_to_target\"] = unfinished_in_degree\n",
    "unfinished_links[\"path_type\"] = \"Unfinished paths\"\n",
    "\n",
    "df_links = pd.concat([finished_links,unfinished_links])\n",
    "\n",
    "ax = sns.boxplot(x=\"path_type\", y=\"links_to_target\", data=df_links, palette=default_colors)\n",
    "plt.xlabel(\" \")\n",
    "plt.ylim([-5,200])\n",
    "plt.ylabel(\"In-degree of target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above highlight these conclusions. The in-degree of targets in the finished paths are noticeably higher than those in the unfinished paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shortest\"></a>\n",
    "\n",
    "#### 4.3 - Analysing Possible Shortest Path Distances in Finished vs Unfinished Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential factor that may determine whether a game will be completed or not, in a more objective manner, is the shortest path length possible between the source and the target. This factor is also intuitive. If a shorter path exists in theory, the path length should also be shorter on average in practice, leading to simpler games. This section explores whether this idea is reflected in the distributions of the length of the shortest possible paths in finished and unfinished games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the finished games.\n",
    "\n",
    "finished_paths[\"shortest_path_length\"] = finished_paths[\"path\"].apply(\n",
    "    lambda x: shortest_paths[articles.loc[articles['article'] == x[0]].index[0]][articles.loc[articles['article'] == x[-1]].index[0]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: There are typos in the targets.\n",
    "\n",
    "Eg. At index 141 in unfinished paths, the target is written as \"Long_peper\", when it should be \"Long_pepper\".\n",
    "\n",
    "Overall, an issue arises in unfinished paths 28 times, but this doesn't seem to be an issue in finished paths. These data points are ignored so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the shortest possible paths for the unfinished games.\n",
    "\n",
    "shortest_unfinished, not_found_unfinished = shortest_path_find(unfinished_paths, articles, shortest_paths)\n",
    "\n",
    "unfinished_paths[\"shortest_path_length\"] = shortest_unfinished\n",
    "print(f\"{not_found_unfinished} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to confirm that there are no issues in the finished paths.\n",
    "\n",
    "shortest_finished, not_found_finished = shortest_path_find(finished_paths, articles, shortest_paths)\n",
    "\n",
    "print(f\"{not_found_finished} shortest paths not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for \"impossible\" paths.\n",
    "\n",
    "print(\"There is {} impossible finished path.\".format(len(finished_paths[finished_paths['shortest_path_length'] == 255])))\n",
    "print(\"There are {} impossible unfinished paths.\".format(len(unfinished_paths[unfinished_paths['shortest_path_length'] == 255])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the case where there were targets with an in-degree of 0, it is likely that \"impossible\" paths exist because the shortest paths matrix provided in the data was not updated after the wikipedia graph kept evolving, as one impossible path was completed. As we determine the shortest possible path length based on that matrix, its errors reflect on our analysis. It is nonetheless believed that the matrix is accurate in the majority of cases, so we simply ignore the \"impossible\" paths in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualise the data below.\n",
    "shortest_possible_finished = finished_paths[finished_paths['shortest_path_length'] != 255][\"shortest_path_length\"]\n",
    "shortest_possible_unfinished = unfinished_paths[unfinished_paths['shortest_path_length'] != 255][\"shortest_path_length\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax1.hist(shortest_possible_finished, bins=[1, 2, 3, 4, 5, 6, 7, 8], color=default_colors[0])\n",
    "ax1.set_xlabel(\"Length of shortest possible path\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Shortest path length possible from source to target in finished paths\")\n",
    "\n",
    "ax2.hist(shortest_possible_unfinished, bins=[1, 2, 3, 4, 5, 6, 7, 8], color=default_colors[0])\n",
    "ax2.set_xlabel(\"Length of shortest possible path\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Shortest path length possible from source to target in unfinished paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of the shortest possible path length makes clear that unfinished paths tend to have longer possible shortest lengths. Intuitively, it makes sense that this would be the case.\n",
    "\n",
    "It also reveals how the shortest possible path length follows a fairly normal distribution, so we may continue with our analysis. The means of the distributions provide sufficient information in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing mean shortest possible paths in the finished and unfinished paths.\n",
    "\n",
    "print(\"The shortest possible paths were {:.3f} long on average in the finished paths.\".format(shortest_possible_finished.mean()))\n",
    "print(\"The shortest possible paths were {:.3f} long on average in the unfinished paths.\".format(shortest_possible_unfinished.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a t test on the shortest path lengths.\n",
    "simple_t_test(shortest_possible_finished, shortest_possible_unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of a t-test between the shortest possible path lengths of finished and unfinished games is 0.0. This means we reject the null hypothesis that the shortest possible game paths are statistically the same across the two groups at the 5% level of significance, and thus the length of the shortest path possible does indeed may have a statistically significant effect on whether a game will be completed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting situation. The past two analyses show that the targets are more difficult to get to in the unfinished paths, due to their lower in-degree and the larger value of the possible shortest path to them.\n",
    "\n",
    "A challenge for us may be to try to isolate whether the difference between whether a path is finished or not can be fully explained by more objective factors like this, or if there is a human component that we can isolate as well, when controlling for factors such as these. Eg, are some categories actually more difficult to get to, or do the differences in the target category distributions in the finished and unfinished paths arise because some categories may be more likely to have longer possible shortest paths to them or have fewer links pointing at them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bck_main\"></a>\n",
    "\n",
    "#### 4.4 - Back click analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to explore the effect of the use of backclicks on the likelihood to complete the search game and reach the target page.<br>\n",
    "In particular, our hypothesis is that the **use of backclics may be an indicator of likely quitting**. It's easy to think that a backclick can mean the user ended up on a page different from what he/she thought, or maybe the backclicked page is missing the links he/she was thinking to found there. Anyway, let's dive into the analysis. \n",
    "\n",
    "NOTE: for the following analyses, we will only consider players who played multiple games, exclude all games shorter than a certain threshold and consider only unfinished games where the player actively quitted, so only the one of type \"restart\". In order to filter out less meaningful data and allows to group data from multiple games played by the same user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter games keeping only data collected from players who played at least 10 games, considering only paths with at least 5 clicks and only restarted games\n",
    "pers_finished_df, pers_unfinished_df = filter_games(finished_paths, unfinished_paths, min_games=10, min_length=5, type=\"restart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bck_distr\"></a>\n",
    "##### 4.4.1. Back-clicks frequency distribution\n",
    "How frequently do players back-click in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two filtered dataframes, adding a column to identify finished and unfinished paths\n",
    "bck_an_all = pd.concat([\n",
    "    pers_finished_df.assign(finished=True),\n",
    "    pers_unfinished_df.assign(finished=False)\n",
    "], ignore_index=True)[[\"hashedIpAddress\", \"path\", \"finished\"]]\n",
    "\n",
    "# Compute backclicked pages (pages a \"<\" correponds to) and visited pages (path without the \"<\")\n",
    "bck_an_all[\"backclicked_pages\"] = bck_an_all[\"path\"].apply(lambda x: get_backclicked_pages(x))\n",
    "bck_an_all[\"visited_pages\"] = bck_an_all[\"path\"].apply(lambda x: [el for el in x if el != \"<\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=bck_an_all[\"backclicked_pages\"].apply(lambda x: len(x) if x is not pd.NA else 0), kde=True, bins=50)\n",
    "plt.title(\"Distribution of backclicked pages per game\")\n",
    "plt.xlabel(\"Number of backclicked pages\")\n",
    "plt.ylabel(\"Number of games\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the seethe distribution is highly skewed since most of the games have 0 or nearly 0 backclicked pages, but there are a few games with both a lot of visited and backclicked pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the impact of the backclicks on the likelyhood of quitting the game, we will then consider the **backclick frequency** per game, defined as the number of backclicked pages divided by the number of visited pages in that path.<br>\n",
    "Let's visualize the back-click frequency distribution in two cases:\n",
    "- first considering all the games independently\n",
    "- then grouping and averaging back-click frequency of same player over multiple games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backclick frequency for each game as the number of backclicked pages divided by the number of visited pages\n",
    "bck_an_all[\"backclick_number\"] = bck_an_all[\"backclicked_pages\"].apply(lambda x: len(x))\n",
    "bck_an_all[\"visited_number\"] = bck_an_all[\"visited_pages\"].apply(lambda x: len(x))\n",
    "\n",
    "bck_an_all[\"backclick_freq\"] = bck_an_all[\"backclick_number\"] / bck_an_all[\"visited_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "\n",
    "sns.histplot(data=bck_an_all[\"backclick_freq\"], kde=True, bins=30, ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of backclick frequency for all games\")\n",
    "axs[0].set_xlabel(\"back-click-rate\")\n",
    "axs[0].set_ylabel(\"Number of games\")\n",
    "\n",
    "sns.histplot(data=bck_an_all[[\"hashedIpAddress\", \"backclick_freq\"]].groupby(\"hashedIpAddress\").mean()[\"backclick_freq\"], kde=True, bins=30, ax=axs[1]) \n",
    "axs[1].set_title(\"Distribution of avg backclick frequency for each player over all their games\")\n",
    "axs[1].set_xlabel(\"Avg backclick-rate\")\n",
    "axs[1].set_ylabel(\"Number of players\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference between the two distributions suggests that the use of backclicks is a characteristic of the player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-freq-compare></a>\n",
    "#### 4.4.2 Back-click frequency distribution in finished vs unfinished paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the average backclick frequency distribution between finished and unfinished paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharey=True, sharex=True)\n",
    "\n",
    "sns.violinplot(bck_an_all[bck_an_all[\"finished\"] == True].groupby(\"hashedIpAddress\")[\"backclick_freq\"].mean(), ax=axs[0], color=default_colors[0], inner=\"quart\")\n",
    "sns.violinplot(bck_an_all[bck_an_all[\"finished\"] == False].groupby(\"hashedIpAddress\")[\"backclick_freq\"].mean(), ax=axs[1], color=default_colors[1], inner=\"quart\")\n",
    "\n",
    "fig.suptitle(\"Distribution of avg backclick frequency for each player over all their games\")\n",
    "axs[0].set_title(\"Finished games\")\n",
    "axs[1].set_title(\"Unfinished games\")\n",
    "axs[0].set_ylabel(\"Avg backclick-frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This violin plot shows how the average backclick frequency is often higher over unfinished paths, giving us the hint this parameter may be an indicator of a more likely quit of the player. <br>\n",
    "Still we'll need further analysis (with a statistically significant test) to draw some more precise conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-rate-cat></a>\n",
    "#### 4.4.3 Backclick rate per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do players backclick more frequently on some categories? Do things change if we consider only finished or unfinished paths?\n",
    "\n",
    "To start exploring possible answers to these questions, let's define the **back-click rate per category** $c$ for a player $P$ as the number of times that $P$ has baclicked on a page that belongs to the category $c$, divide by the number of times $P$ visited a page of that category. This measure should express how likely is the player $P$ to back-click a page of category $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to count the number of backclicks and visits for each category\n",
    "bck_an_all[\"visited_categories\"] = bck_an_all[\"visited_pages\"].apply(lambda x: [article_to_broad_category[page][0] for page in x if page in article_to_broad_category.keys()])\n",
    "bck_an_all[\"backclicked_categories\"] = bck_an_all[\"backclicked_pages\"].apply(lambda x: [article_to_broad_category[page][0] for page in x if page in article_to_broad_category.keys()]\n",
    "                                                                                        if x is not pd.NA else pd.NA)\n",
    "\n",
    "backclicks_per_category = pd.merge(\n",
    "    bck_an_all[[\"hashedIpAddress\", \"visited_categories\", \"finished\"]].explode(\"visited_categories\").groupby([\"hashedIpAddress\", \"visited_categories\", \"finished\"]).size().reset_index(name=\"visits_count\"),\n",
    "    bck_an_all[[\"hashedIpAddress\", \"backclicked_categories\", \"finished\"]].explode(\"backclicked_categories\").groupby([\"hashedIpAddress\", \"backclicked_categories\", \"finished\"]).size().reset_index(name=\"backclicks_count\"),\n",
    "    how=\"left\",\n",
    "    left_on=[\"hashedIpAddress\", \"visited_categories\", \"finished\"],\n",
    "    right_on=[\"hashedIpAddress\", \"backclicked_categories\", \"finished\"],\n",
    ")\n",
    "\n",
    "backclicks_per_category.drop(columns=[\"backclicked_categories\"], inplace=True)\n",
    "backclicks_per_category.rename(columns={\"visited_categories\": \"category\"}, inplace=True)\n",
    "backclicks_per_category[\"backclicks_count\"].fillna(0, inplace=True)\n",
    "backclicks_per_category[\"backclick_rate_per_category\"] = backclicks_per_category[\"backclicks_count\"] / backclicks_per_category[\"visits_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a paired-column bar chart, showing for each category the average backclick-rate for finished and unfinished games\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=backclicks_per_category[[\"category\", \"finished\", \"backclick_rate_per_category\"]],\n",
    "            x=\"category\",\n",
    "            y=\"backclick_rate_per_category\",\n",
    "            hue=\"finished\",\n",
    "            order=backclicks_per_category[backclicks_per_category[\"finished\"] == False][[\"category\", \"backclick_rate_per_category\"]].groupby(\"category\").mean().sort_values(by=\"backclick_rate_per_category\", ascending=False).index,\n",
    "            hue_order=[True, False],\n",
    "            errorbar=(\"ci\", 95),\n",
    "            errwidth=1.4,\n",
    "            capsize=0.1,\n",
    "            estimator=np.mean,)\n",
    "plt.xticks(rotation=80)\n",
    "plt.title(\"Avg backclick-rate per category: finished vs unfinished games\")\n",
    "plt.ylabel(\"Avg backclick-rate per category\", labelpad=20)\n",
    "plt.xlabel(\"Category\", labelpad=20)\n",
    "plt.legend(title=\"Finished game\", ncols=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar chart shows that the average back-click rate is often significantly different for finished and unfinished paths.\n",
    "Since this data compare finished and unfinished per-category backclicl-rate of all players, we could see that it's likely that a player with a high back-click rate will quit, even if the measure of how likely is this case, depends on the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bck-test></a>\n",
    "#### 4.4.4 Backclick frequency impact on finishing the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to the starting point: is there a significant relationship between the backclick frequency over a game and the fact this game may have ended or not?\n",
    "\n",
    "A first answer can be found by looking to the [**point biserial correlation**](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient), which is a correlation coefficient, mathematically equivalent to the Pearson correlation, specific for cases where we have one continouous variable and a dichotomous one. <br>\n",
    "This setting fits our needs, in particular we can consider the following random variables:\n",
    "- $X=\\lbrace1 \\text{ if the game is finished, } 0\\text{ if the game is not finished}\\rbrace$ $\\implies$ dichotomous variable\n",
    "- $Y=\\text{backclick\\_frequency}$ $\\implies$ continuous variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider a simple case where all the different games are considered independently each other, so for this first analysis we won't group games played by the same user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(x=bck_an_all[\"finished\"], y=bck_an_all[\"backclick_freq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see a *small* negative correlation which is statistically significant. <br>\n",
    "In particular, the fact that the correlation is negative already tells us that the higher the $\\text{backclick\\_frequency}$ the lower the probability of finishing the game $\\Bbb{P}(X=1)$.\n",
    "\n",
    "Let's now group data from different games of the same player, in particular, we'll take for each player it's average $\\text{backclick\\_frequency}$ over his/her finished games and over his/her unfinished games. <br>\n",
    "By doing this we'll be able to tell whether there is a significant difference in the average backclick rate of each player, between his/her finished vs. unfinished games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(\n",
    "    x=bck_an_all.groupby([\"hashedIpAddress\", \"finished\"])[\"backclick_freq\"].mean().reset_index()[\"finished\"],\n",
    "    y=bck_an_all.groupby([\"hashedIpAddress\", \"finished\"])[\"backclick_freq\"].mean().reset_index()[\"backclick_freq\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the correlation is even higher (in absolute value) and still statistically significant, leading us to think that the backclick frequency of a player may be an indicator of likely quitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint1\"></a>\n",
    "\n",
    "## Checkpoint for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code saves the version of the dataframe above:\n",
    "finished_paths.to_pickle(os.path.join(GENERATED_METRICS, \"finished_paths_initial_stats.pkl\"))\n",
    "unfinished_paths.to_pickle(os.path.join(GENERATED_METRICS, \"unfinished_paths_initial_stats.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code reads that version of the dataframe from the file:\n",
    "finished_paths = pd.read_pickle(os.path.join(GENERATED_METRICS, \"finished_paths_initial_stats.pkl\"))\n",
    "unfinished_paths = pd.read_pickle(os.path.join(GENERATED_METRICS, \"unfinished_paths_initial_stats.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"everything\"></a>\n",
    "\n",
    "## 5 - Putting Everything Together\n",
    "### Building a Logistic Regression  to determine influencing factors on the propensity of a player to give up a game (restart or timeout)\n",
    "We combine all factors we have explored before to build a regression model to predict whether a player gives up to then interpret the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we first merge the article metrics (categories, word count etc.) to the finished and unfinished paths to create a dataset\n",
    "\n",
    "# merge and unfinished paths while adding a flag\n",
    "finished_paths[\"give_up\"] = 0\n",
    "unfinished_paths[\"give_up\"] = 1\n",
    "games = pd.concat((finished_paths, unfinished_paths), axis=0)\n",
    "\n",
    "# drop duplicates in the article column, since one article might belong to more than one main category, some rows are duplicated. For now, we just drop these\n",
    "article_metrics_with_categories = article_metrics_with_categories.drop_duplicates(subset=\"article\")\n",
    "\n",
    "# define columns that may be relevant from article metrics:\n",
    "# some columns are exluded since they are contained in others, or because they are complements (stopword vs non-stopword percentage)\n",
    "keep = ['article',\n",
    "        'broad_category',\n",
    "        'paragraph_count',\n",
    "        'readability_score',\n",
    "        \"stopword_percentage\",\n",
    "        'avg_word_length',\n",
    "        'avg_sent_length',\n",
    "        ]\n",
    "\n",
    "# merge on start\n",
    "start_metrics = article_metrics_with_categories[keep].add_prefix(\"start_\")\n",
    "games = pd.merge(games, start_metrics, how=\"left\", left_on=\"start\", right_on=\"start_article\") # add prefix\n",
    "print(games.shape) # check results of the merge\n",
    "\n",
    "# merge on target\n",
    "target_metrics = article_metrics_with_categories[keep].add_prefix(\"target_\")\n",
    "games = pd.merge(games, target_metrics, how=\"left\", left_on=\"target\", right_on=\"target_article\") # add prefix\n",
    "print(games.shape) # check results of the merge\n",
    "\n",
    "# subset games to only include those with reasonable path lengths - TBD, no filtering for now\n",
    "# games = games[(games.path_length > 1) & (games.path_length < 30)]\n",
    "\n",
    "\n",
    "# remove unnecessary columns\n",
    "to_drop = ['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating',\n",
    "       'datetime', 'start', 'target', 'path_length', \"target_article\", \"start_article\", \"type\",]\n",
    "games = games.drop(to_drop, axis=1)\n",
    "print(games.shape) # check results of the subsetting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = games.isnull().sum() * 100 / len(games)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NAs - for the actual analysis we need to investigate more why these are coming from.\n",
    "# for this proof of concept we just remove them\n",
    "games = games.dropna(axis=0, how=\"any\")\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create formula for logistic regression\n",
    "target = \"give_up\"\n",
    "predictors = [col for col in games.columns if col != target]\n",
    "formula = target + \" ~ \" + \" + \".join(predictors)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model with the full formula (i.e. all relevant predictors)\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression model with limited predictors: only those visible to a player at the start of the game\n",
    "formula = 'give_up ~ links_to_target + shortest_path_length + start_broad_category + start_paragraph_count + start_readability_score + start_stopword_percentage + start_avg_sent_length + C(target_broad_category)'\n",
    "mod = smf.logit(formula=formula, data=games)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two regression models are largely congruent and offer some interesting initial findings (non-exhaustive):\n",
    "- some categories have a large and statistically significant influence on the proabability of a player giving up. A few examples:\n",
    "    - paths starting from *language and literature* or more niche topics like *Design and Technology* increase the propensity to give up\n",
    "    - a target article in the categories *Geography* or *Countries* strongly decreases the probability. This is consistent with the hypothesis that these are rather \"easy\" categories, as many links point to them.\n",
    "- similarly, many of the article related metrics are statistically significant; for instance:\n",
    "    - the shortest_path_length has a large positive coefficient, indicating that objectively longer paths do lead to more failures\n",
    "    - more detailed article metrics are statistically relevant, but the effect sizes are quite small (e.g., in-degree of target, readability score etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
